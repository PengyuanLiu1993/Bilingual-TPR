{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Geoparsing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99473339-1402-4ee9-f14c-80bb67b88015"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 6.8MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 27.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting torch<=1.7.1,>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 24kB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/02/be/4dd30d56a0a19619deb9bf41ba8202709fa83b1b301b876572cd6dc38117/konoha-4.6.4-py3-none-any.whl\n",
            "Collecting transformers>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 52.7MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.2MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 48.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->flair) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.0.12)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.10.1)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->flair) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9693 sha256=3fc5693110d48aa95efa146dcf64b1768c6f8a8dc4c4b6819ea07722444fcf44\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
            "Successfully built gdown\n",
            "Building wheels for collected packages: mpld3, sqlitedict, ftfy, langdetect, segtok, overrides\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116679 sha256=72ae9fd97ea3b247732409e93f4dbff2f453a43bb9cd2799686f0efa2ce3e392\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14376 sha256=10c7ab2380d4b22e5da2b0e1f0e98c98ddc5c7c5495c67fb58b340b397265150\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=46d3c439069b9aa853c832eb52dad0583b69d603d25baaf91ac1950d262f32b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp37-none-any.whl size=993193 sha256=2b6cca3763b9e3e98b5dc834c63475f83e4a7c9a6125bc9d4c4b3875817843cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=0665627dff56c1386e2eb6e2585e762285b02c5e74692d57d0e27cf84c36d811\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=091f53f51cfcea70a3a29b7440333c150ccc944e618ca27e66b7bbbfa812eb5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "Successfully built mpld3 sqlitedict ftfy langdetect segtok overrides\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.4 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: deprecated, gdown, mpld3, torch, sqlitedict, ftfy, huggingface-hub, overrides, konoha, tokenizers, sacremoses, transformers, janome, langdetect, sentencepiece, bpemb, segtok, flair\n",
            "  Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.1 gdown-3.12.2 huggingface-hub-0.0.8 janome-0.4.1 konoha-4.6.4 langdetect-1.0.8 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.2 torch-1.7.1 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JeraOrHf1H21",
        "outputId": "cc1dd445-4aef-4d7f-970f-112a25069e79"
      },
      "source": [
        "!pip install allennlp==0.9.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.1.2)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.4.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.7.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (4.41.1)\n",
            "Collecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/85/df3d1fd2b60a87455475f93012861b76a411d27ba4a0859939adbe2c9dc3/gevent-21.1.2-cp37-cp37m-manylinux2010_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 32.0MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (2018.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.19.5)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.5.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.2.5)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 59.4MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 52.5MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/8f/42959300c543b4d34bc9f9b54954471a33384c181084ed84f070763d7f37/boto3-1.17.62-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 60.3MB/s \n",
            "\u001b[?25hCollecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (6.0.1)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/00/0e63b7024c2d873bf57411ab0ed77eeafd5f44bace7cbf1d56bca8ab3be2/responses-0.13.3-py2.py3-none-any.whl\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 52.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (2.23.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.2.2)\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/f3/554271be8ff46471586d164bfbb6999364ba30ca5a0045e2a86da5f3b2c5/spacy-2.1.9-cp37-cp37m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.8MB 112kB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.6.4)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (2.10.0)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.4MB/s \n",
            "\u001b[?25hCollecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.4.1)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.1.0)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 51.6MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.22.2.post1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->allennlp==0.9.0) (3.7.4.3)\n",
            "Collecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp==0.9.0) (56.0.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp==0.9.0) (1.0.0)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/a7/94e1a92c71436f934cdd2102826fa041c83dcb7d21dd0f1fb1a57f6e0620/zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp==0.9.0) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp==0.9.0) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->allennlp==0.9.0) (0.1.95)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.62\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/60/ba830f93176fdc23166043298173ee2aecd5cf150f1ede51d6506f021deb/botocore-1.20.62-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp==0.9.0) (0.2.5)\n",
            "Collecting urllib3>=1.25.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 59.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp==0.9.0) (3.12.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==0.9.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==0.9.0) (2020.12.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (2.8.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9.0) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9.0) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9.0) (2.0.5)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.8MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.0MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.1.0->allennlp==0.9.0) (1.0.5)\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp==0.9.0) (3.10.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (8.7.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (1.10.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp==0.9.0) (1.8.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==0.9.0) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp==0.9.0) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==0.9.0) (3.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.9.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.2.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (0.7.12)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.6.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.1.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (0.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (20.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.1.4)\n",
            "Building wheels for collected packages: parsimonious, jsonnet, word2number\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp37-none-any.whl size=42711 sha256=c8d31af7a6952639b595eeabb33dfb1c13ce1eec0e19b9f8f23c4683adb569ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388792 sha256=4f10995485fa9214fd46d7c3bb53508173bfdc993e6a2e603275d3a29c879451\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5589 sha256=ffd4255b1531a501ab81c01c90a2a9f77f1b1bb1411e6c1d244171f4953ad93e\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "Successfully built parsimonious jsonnet word2number\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: konoha 4.6.4 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: flaky, zope.event, zope.interface, gevent, jmespath, urllib3, botocore, s3transfer, boto3, pytorch-pretrained-bert, pytorch-transformers, unidecode, flask-cors, responses, tensorboardX, blis, preshed, plac, thinc, spacy, jsonpickle, conllu, parsimonious, numpydoc, jsonnet, word2number, allennlp\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 boto3-1.17.62 botocore-1.20.62 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.10 gevent-21.1.2 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 numpydoc-1.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.13.3 s3transfer-0.4.2 spacy-2.1.9 tensorboardX-2.2 thinc-7.0.8 unidecode-1.2.0 urllib3-1.26.4 word2number-1.1 zope.event-4.5.0 zope.interface-5.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5695d40-1199-43e0-efaa-a36dc252aea2"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "columns = {0:'text',1:'ner'}\n",
        "\n",
        "data_folder = '/content/Data'\n",
        "\n",
        "corpus:Corpus=ColumnCorpus(data_folder,columns,train_file='fi_en_train.txt',test_file='fi_en_test.txt')\n",
        "corpus = corpus.downsample(0.8)\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-03 10:41:25,343 Reading data from /content/Data\n",
            "2021-05-03 10:41:25,344 Train: /content/Data/fi_en_train.txt\n",
            "2021-05-03 10:41:25,349 Dev: None\n",
            "2021-05-03 10:41:25,351 Test: /content/Data/fi_en_test.txt\n",
            "Corpus: 5128 train + 570 dev + 680 test sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S21_5-y7Z8AS",
        "outputId": "36248be0-e13b-46bf-e7e7-9f4e5f722d1b"
      },
      "source": [
        "tag_type = 'ner'# make tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary with 7 tags: <unk>, O, B-location, I-location, , <START>, <STOP>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqKxKDrjaAd4"
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, StackedEmbeddings, CharacterEmbeddings, MuseCrosslingualEmbeddings, ELMoEmbeddings\n",
        "from typing import List\n",
        "\n",
        "from flair.embeddings import FlairEmbeddings, TransformerWordEmbeddings,TokenEmbeddings\n",
        "\n",
        "word_embedding = MuseCrosslingualEmbeddings()\n",
        "\n",
        "bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased')\n",
        "embeddings_types= StackedEmbeddings(embeddings=[CharacterEmbeddings(), bert_embedding])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRbfDopeaEAg",
        "outputId": "42fdaa47-6383-4ff5-ed04-94367e5bab86"
      },
      "source": [
        "from flair.models import SequenceTagger\n",
        "tagger : SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                       embeddings=embeddings_types,\n",
        "                                       tag_dictionary=tag_dictionary,\n",
        "                                       tag_type=tag_type,\n",
        "                                       use_crf=True)\n",
        "# from flair.models import SequenceTagger\n",
        "\n",
        "# tagger = SequenceTagger.load('ner')\n",
        "print(tagger)\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(count_parameters(tagger))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SequenceTagger(\n",
            "  (embeddings): TransformerWordEmbeddings(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=7, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\n",
            "180548920\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogglr_LRaGeJ",
        "outputId": "a6d6d797-a07e-4935-c4d2-1cb7da18ed5d"
      },
      "source": [
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer : ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train('resources/taggers/example-ner',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              # train_with_dev=True,\n",
        "              # monitor_test=True,\n",
        "              max_epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 12:29:40,118 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:29:40,122 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): MuseCrosslingualEmbeddings()\n",
            "    (list_embedding_1): CharacterEmbeddings(\n",
            "      (char_embedding): Embedding(275, 25)\n",
            "      (char_rnn): LSTM(25, 25, bidirectional=True)\n",
            "    )\n",
            "    (list_embedding_2): TransformerWordEmbeddings(\n",
            "      (model): BertModel(\n",
            "        (embeddings): BertEmbeddings(\n",
            "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (encoder): BertEncoder(\n",
            "          (layer): ModuleList(\n",
            "            (0): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (1): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (2): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (3): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (4): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (5): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (6): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (7): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (8): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (9): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (10): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (11): BertLayer(\n",
            "              (attention): BertAttention(\n",
            "                (self): BertSelfAttention(\n",
            "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (output): BertSelfOutput(\n",
            "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (intermediate): BertIntermediate(\n",
            "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              )\n",
            "              (output): BertOutput(\n",
            "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (pooler): BertPooler(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (activation): Tanh()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=1118, out_features=1118, bias=True)\n",
            "  (rnn): LSTM(1118, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=7, bias=True)\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2021-03-11 12:29:40,123 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:29:40,124 Corpus: \"Corpus: 5128 train + 570 dev + 680 test sentences\"\n",
            "2021-03-11 12:29:40,125 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:29:40,125 Parameters:\n",
            "2021-03-11 12:29:40,126  - learning_rate: \"0.1\"\n",
            "2021-03-11 12:29:40,128  - mini_batch_size: \"32\"\n",
            "2021-03-11 12:29:40,129  - patience: \"3\"\n",
            "2021-03-11 12:29:40,130  - anneal_factor: \"0.5\"\n",
            "2021-03-11 12:29:40,131  - max_epochs: \"150\"\n",
            "2021-03-11 12:29:40,131  - shuffle: \"True\"\n",
            "2021-03-11 12:29:40,132  - train_with_dev: \"False\"\n",
            "2021-03-11 12:29:40,133  - batch_growth_annealing: \"False\"\n",
            "2021-03-11 12:29:40,138 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:29:40,140 Model training base path: \"resources/taggers/example-ner\"\n",
            "2021-03-11 12:29:40,141 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:29:40,144 Device: cuda:0\n",
            "2021-03-11 12:29:40,144 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:29:40,145 Embeddings storage mode: cpu\n",
            "2021-03-11 12:29:40,156 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:29:40,492 Loading up MUSE embeddings for 'en'!\n",
            "2021-03-11 12:29:47,784 Loading up MUSE embeddings for 'de'!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/flair/embeddings/token.py:1580: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  word_embedding, device=flair.device, dtype=torch.float\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 12:30:03,707 Loading up MUSE embeddings for 'hr'!\n",
            "2021-03-11 12:30:16,360 epoch 1 - iter 16/161 - loss 14.76506644 - samples/sec: 14.14 - lr: 0.100000\n",
            "2021-03-11 12:30:30,536 epoch 1 - iter 32/161 - loss 10.68159711 - samples/sec: 36.12 - lr: 0.100000\n",
            "2021-03-11 12:30:30,683 Loading up MUSE embeddings for 'no'!\n",
            "2021-03-11 12:30:51,539 epoch 1 - iter 48/161 - loss 8.32348785 - samples/sec: 24.38 - lr: 0.100000\n",
            "2021-03-11 12:30:55,970 Loading up MUSE embeddings for 'ro'!\n",
            "2021-03-11 12:31:10,547 Loading up MUSE embeddings for 'ca'!\n",
            "2021-03-11 12:31:18,393 Loading up MUSE embeddings for 'fr'!\n",
            "2021-03-11 12:31:27,652 epoch 1 - iter 64/161 - loss 7.00176718 - samples/sec: 14.18 - lr: 0.100000\n",
            "2021-03-11 12:31:40,685 Loading up MUSE embeddings for 'id'!\n",
            "2021-03-11 12:31:41,165 https://flair.informatik.hu-berlin.de/resources/embeddings/muse/muse.id.vec.gensim.vectors.npy not found in cache, downloading to /tmp/tmp0hjann21\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 240000080/240000080 [00:14<00:00, 16272247.64B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 12:31:56,420 copying /tmp/tmp0hjann21 to cache at /root/.flair/embeddings/MUSE/muse.id.vec.gensim.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 12:31:58,517 removing temp file /tmp/tmp0hjann21\n",
            "2021-03-11 12:32:02,791 https://flair.informatik.hu-berlin.de/resources/embeddings/muse/muse.id.vec.gensim not found in cache, downloading to /tmp/tmpocm6itxs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10614113/10614113 [00:01<00:00, 7107204.24B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 12:32:04,771 copying /tmp/tmpocm6itxs to cache at /root/.flair/embeddings/MUSE/muse.id.vec.gensim\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 12:32:04,784 removing temp file /tmp/tmpocm6itxs\n",
            "2021-03-11 12:32:06,248 epoch 1 - iter 80/161 - loss 6.08155067 - samples/sec: 13.27 - lr: 0.100000\n",
            "2021-03-11 12:32:17,318 Loading up MUSE embeddings for 'da'!\n",
            "2021-03-11 12:32:28,550 epoch 1 - iter 96/161 - loss 5.62066316 - samples/sec: 22.96 - lr: 0.100000\n",
            "2021-03-11 12:32:30,828 Loading up MUSE embeddings for 'fi'!\n",
            "2021-03-11 12:32:47,814 Loading up MUSE embeddings for 'et'!\n",
            "2021-03-11 12:32:55,671 Loading up MUSE embeddings for 'it'!\n",
            "2021-03-11 12:33:05,318 epoch 1 - iter 112/161 - loss 5.01753703 - samples/sec: 13.93 - lr: 0.100000\n",
            "2021-03-11 12:33:16,400 Loading up MUSE embeddings for 'nl'!\n",
            "2021-03-11 12:33:26,917 epoch 1 - iter 128/161 - loss 4.45110999 - samples/sec: 23.71 - lr: 0.100000\n",
            "2021-03-11 12:33:41,454 epoch 1 - iter 144/161 - loss 3.98645792 - samples/sec: 35.23 - lr: 0.100000\n",
            "2021-03-11 12:33:56,043 epoch 1 - iter 160/161 - loss 3.61502726 - samples/sec: 35.10 - lr: 0.100000\n",
            "2021-03-11 12:33:56,331 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:33:56,331 EPOCH 1 done: loss 3.5926 - lr 0.1000000\n",
            "2021-03-11 12:34:09,688 DEV : loss 2.9712154865264893 - score 0.6634\n",
            "2021-03-11 12:34:09,726 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-03-11 12:35:48,164 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:35:54,021 epoch 2 - iter 16/161 - loss 1.93819320 - samples/sec: 87.62 - lr: 0.100000\n",
            "2021-03-11 12:35:59,943 epoch 2 - iter 32/161 - loss 1.77226946 - samples/sec: 86.49 - lr: 0.100000\n",
            "2021-03-11 12:36:05,785 epoch 2 - iter 48/161 - loss 1.69037832 - samples/sec: 87.68 - lr: 0.100000\n",
            "2021-03-11 12:36:11,692 epoch 2 - iter 64/161 - loss 1.69046498 - samples/sec: 86.71 - lr: 0.100000\n",
            "2021-03-11 12:36:17,697 epoch 2 - iter 80/161 - loss 1.65080115 - samples/sec: 85.29 - lr: 0.100000\n",
            "2021-03-11 12:36:23,525 epoch 2 - iter 96/161 - loss 1.61854747 - samples/sec: 87.89 - lr: 0.100000\n",
            "2021-03-11 12:36:29,330 epoch 2 - iter 112/161 - loss 1.56846366 - samples/sec: 88.24 - lr: 0.100000\n",
            "2021-03-11 12:36:35,222 epoch 2 - iter 128/161 - loss 1.55948759 - samples/sec: 86.93 - lr: 0.100000\n",
            "2021-03-11 12:36:41,082 epoch 2 - iter 144/161 - loss 1.52165543 - samples/sec: 87.41 - lr: 0.100000\n",
            "2021-03-11 12:36:46,954 epoch 2 - iter 160/161 - loss 1.50654001 - samples/sec: 87.24 - lr: 0.100000\n",
            "2021-03-11 12:36:47,084 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:36:47,085 EPOCH 2 done: loss 1.5052 - lr 0.1000000\n",
            "2021-03-11 12:36:49,651 DEV : loss 0.9905304312705994 - score 0.8622\n",
            "2021-03-11 12:36:49,690 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-03-11 12:38:23,056 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:38:30,198 epoch 3 - iter 16/161 - loss 1.32162252 - samples/sec: 86.86 - lr: 0.100000\n",
            "2021-03-11 12:38:36,065 epoch 3 - iter 32/161 - loss 1.25594273 - samples/sec: 87.31 - lr: 0.100000\n",
            "2021-03-11 12:38:41,881 epoch 3 - iter 48/161 - loss 1.22449205 - samples/sec: 88.08 - lr: 0.100000\n",
            "2021-03-11 12:38:47,787 epoch 3 - iter 64/161 - loss 1.24500282 - samples/sec: 86.73 - lr: 0.100000\n",
            "2021-03-11 12:38:53,696 epoch 3 - iter 80/161 - loss 1.27635287 - samples/sec: 86.69 - lr: 0.100000\n",
            "2021-03-11 12:38:59,559 epoch 3 - iter 96/161 - loss 1.28346658 - samples/sec: 87.35 - lr: 0.100000\n",
            "2021-03-11 12:39:05,485 epoch 3 - iter 112/161 - loss 1.26268488 - samples/sec: 86.44 - lr: 0.100000\n",
            "2021-03-11 12:39:11,288 epoch 3 - iter 128/161 - loss 1.24775165 - samples/sec: 88.26 - lr: 0.100000\n",
            "2021-03-11 12:39:17,110 epoch 3 - iter 144/161 - loss 1.21986035 - samples/sec: 87.97 - lr: 0.100000\n",
            "2021-03-11 12:39:22,952 epoch 3 - iter 160/161 - loss 1.21237137 - samples/sec: 87.68 - lr: 0.100000\n",
            "2021-03-11 12:39:23,075 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:39:23,076 EPOCH 3 done: loss 1.2072 - lr 0.1000000\n",
            "2021-03-11 12:39:25,618 DEV : loss 0.8376005291938782 - score 0.8967\n",
            "2021-03-11 12:39:25,658 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-03-11 12:40:58,213 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:41:06,410 epoch 4 - iter 16/161 - loss 1.14639032 - samples/sec: 87.46 - lr: 0.100000\n",
            "2021-03-11 12:41:12,263 epoch 4 - iter 32/161 - loss 1.15546946 - samples/sec: 87.51 - lr: 0.100000\n",
            "2021-03-11 12:41:18,014 epoch 4 - iter 48/161 - loss 1.12498201 - samples/sec: 89.07 - lr: 0.100000\n",
            "2021-03-11 12:41:23,736 epoch 4 - iter 64/161 - loss 1.07007525 - samples/sec: 89.52 - lr: 0.100000\n",
            "2021-03-11 12:41:29,719 epoch 4 - iter 80/161 - loss 1.07187813 - samples/sec: 85.62 - lr: 0.100000\n",
            "2021-03-11 12:41:35,703 epoch 4 - iter 96/161 - loss 1.05962088 - samples/sec: 85.60 - lr: 0.100000\n",
            "2021-03-11 12:41:41,611 epoch 4 - iter 112/161 - loss 1.06657408 - samples/sec: 86.69 - lr: 0.100000\n",
            "2021-03-11 12:41:47,528 epoch 4 - iter 128/161 - loss 1.06025764 - samples/sec: 86.58 - lr: 0.100000\n",
            "2021-03-11 12:41:53,389 epoch 4 - iter 144/161 - loss 1.05363634 - samples/sec: 87.39 - lr: 0.100000\n",
            "2021-03-11 12:41:59,217 epoch 4 - iter 160/161 - loss 1.05504047 - samples/sec: 87.89 - lr: 0.100000\n",
            "2021-03-11 12:41:59,333 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:41:59,334 EPOCH 4 done: loss 1.0530 - lr 0.1000000\n",
            "2021-03-11 12:42:01,869 DEV : loss 1.0136915445327759 - score 0.8835\n",
            "2021-03-11 12:42:01,908 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 12:42:01,915 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:42:07,808 epoch 5 - iter 16/161 - loss 1.16839653 - samples/sec: 86.95 - lr: 0.100000\n",
            "2021-03-11 12:42:13,558 epoch 5 - iter 32/161 - loss 1.08708532 - samples/sec: 89.08 - lr: 0.100000\n",
            "2021-03-11 12:42:19,430 epoch 5 - iter 48/161 - loss 1.02157375 - samples/sec: 87.23 - lr: 0.100000\n",
            "2021-03-11 12:42:25,209 epoch 5 - iter 64/161 - loss 1.05488995 - samples/sec: 88.64 - lr: 0.100000\n",
            "2021-03-11 12:42:31,219 epoch 5 - iter 80/161 - loss 1.02794158 - samples/sec: 85.23 - lr: 0.100000\n",
            "2021-03-11 12:42:36,950 epoch 5 - iter 96/161 - loss 0.98661695 - samples/sec: 89.38 - lr: 0.100000\n",
            "2021-03-11 12:42:42,732 epoch 5 - iter 112/161 - loss 0.97399002 - samples/sec: 88.59 - lr: 0.100000\n",
            "2021-03-11 12:42:48,619 epoch 5 - iter 128/161 - loss 0.97921126 - samples/sec: 87.01 - lr: 0.100000\n",
            "2021-03-11 12:42:54,578 epoch 5 - iter 144/161 - loss 0.96446140 - samples/sec: 85.96 - lr: 0.100000\n",
            "2021-03-11 12:43:00,435 epoch 5 - iter 160/161 - loss 0.96712225 - samples/sec: 87.44 - lr: 0.100000\n",
            "2021-03-11 12:43:00,554 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:43:00,555 EPOCH 5 done: loss 0.9619 - lr 0.1000000\n",
            "2021-03-11 12:43:03,106 DEV : loss 0.7377455830574036 - score 0.9082\n",
            "2021-03-11 12:43:03,145 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-03-11 12:44:33,890 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:44:41,803 epoch 6 - iter 16/161 - loss 0.93386345 - samples/sec: 87.87 - lr: 0.100000\n",
            "2021-03-11 12:44:47,640 epoch 6 - iter 32/161 - loss 0.89426257 - samples/sec: 87.75 - lr: 0.100000\n",
            "2021-03-11 12:44:53,560 epoch 6 - iter 48/161 - loss 0.88892611 - samples/sec: 86.53 - lr: 0.100000\n",
            "2021-03-11 12:44:59,408 epoch 6 - iter 64/161 - loss 0.92769169 - samples/sec: 87.59 - lr: 0.100000\n",
            "2021-03-11 12:45:05,293 epoch 6 - iter 80/161 - loss 0.93201528 - samples/sec: 87.04 - lr: 0.100000\n",
            "2021-03-11 12:45:11,106 epoch 6 - iter 96/161 - loss 0.92357176 - samples/sec: 88.11 - lr: 0.100000\n",
            "2021-03-11 12:45:16,878 epoch 6 - iter 112/161 - loss 0.92501213 - samples/sec: 88.74 - lr: 0.100000\n",
            "2021-03-11 12:45:22,624 epoch 6 - iter 128/161 - loss 0.92603353 - samples/sec: 89.14 - lr: 0.100000\n",
            "2021-03-11 12:45:28,386 epoch 6 - iter 144/161 - loss 0.91513899 - samples/sec: 88.90 - lr: 0.100000\n",
            "2021-03-11 12:45:34,179 epoch 6 - iter 160/161 - loss 0.92323726 - samples/sec: 88.41 - lr: 0.100000\n",
            "2021-03-11 12:45:34,302 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:45:34,303 EPOCH 6 done: loss 0.9254 - lr 0.1000000\n",
            "2021-03-11 12:45:36,832 DEV : loss 0.7242583632469177 - score 0.9093\n",
            "2021-03-11 12:45:36,870 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-03-11 12:47:08,312 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:47:17,198 epoch 7 - iter 16/161 - loss 0.74388073 - samples/sec: 86.49 - lr: 0.100000\n",
            "2021-03-11 12:47:22,989 epoch 7 - iter 32/161 - loss 0.83501174 - samples/sec: 88.45 - lr: 0.100000\n",
            "2021-03-11 12:47:28,733 epoch 7 - iter 48/161 - loss 0.90868424 - samples/sec: 89.17 - lr: 0.100000\n",
            "2021-03-11 12:47:34,601 epoch 7 - iter 64/161 - loss 0.89712076 - samples/sec: 87.29 - lr: 0.100000\n",
            "2021-03-11 12:47:40,336 epoch 7 - iter 80/161 - loss 0.87527579 - samples/sec: 89.32 - lr: 0.100000\n",
            "2021-03-11 12:47:46,305 epoch 7 - iter 96/161 - loss 0.88473467 - samples/sec: 85.81 - lr: 0.100000\n",
            "2021-03-11 12:47:52,129 epoch 7 - iter 112/161 - loss 0.88600622 - samples/sec: 87.95 - lr: 0.100000\n",
            "2021-03-11 12:47:57,953 epoch 7 - iter 128/161 - loss 0.88197290 - samples/sec: 87.94 - lr: 0.100000\n",
            "2021-03-11 12:48:03,862 epoch 7 - iter 144/161 - loss 0.86994260 - samples/sec: 86.69 - lr: 0.100000\n",
            "2021-03-11 12:48:09,702 epoch 7 - iter 160/161 - loss 0.85806676 - samples/sec: 87.71 - lr: 0.100000\n",
            "2021-03-11 12:48:09,827 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:48:09,828 EPOCH 7 done: loss 0.8615 - lr 0.1000000\n",
            "2021-03-11 12:48:12,391 DEV : loss 0.6750032305717468 - score 0.9064\n",
            "2021-03-11 12:48:12,430 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 12:48:12,432 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:48:18,378 epoch 8 - iter 16/161 - loss 0.62228095 - samples/sec: 86.18 - lr: 0.100000\n",
            "2021-03-11 12:48:24,385 epoch 8 - iter 32/161 - loss 0.73684236 - samples/sec: 85.27 - lr: 0.100000\n",
            "2021-03-11 12:48:30,267 epoch 8 - iter 48/161 - loss 0.83154747 - samples/sec: 87.07 - lr: 0.100000\n",
            "2021-03-11 12:48:36,081 epoch 8 - iter 64/161 - loss 0.83897430 - samples/sec: 88.11 - lr: 0.100000\n",
            "2021-03-11 12:48:41,896 epoch 8 - iter 80/161 - loss 0.82258641 - samples/sec: 88.08 - lr: 0.100000\n",
            "2021-03-11 12:48:47,714 epoch 8 - iter 96/161 - loss 0.84228692 - samples/sec: 88.05 - lr: 0.100000\n",
            "2021-03-11 12:48:53,713 epoch 8 - iter 112/161 - loss 0.84879655 - samples/sec: 85.38 - lr: 0.100000\n",
            "2021-03-11 12:48:59,583 epoch 8 - iter 128/161 - loss 0.84152621 - samples/sec: 87.25 - lr: 0.100000\n",
            "2021-03-11 12:49:05,462 epoch 8 - iter 144/161 - loss 0.83196344 - samples/sec: 87.12 - lr: 0.100000\n",
            "2021-03-11 12:49:11,298 epoch 8 - iter 160/161 - loss 0.84252767 - samples/sec: 87.77 - lr: 0.100000\n",
            "2021-03-11 12:49:11,430 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:49:11,431 EPOCH 8 done: loss 0.8435 - lr 0.1000000\n",
            "2021-03-11 12:49:13,998 DEV : loss 0.6758297085762024 - score 0.9063\n",
            "2021-03-11 12:49:14,036 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 12:49:14,037 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:49:19,876 epoch 9 - iter 16/161 - loss 0.76425242 - samples/sec: 87.75 - lr: 0.100000\n",
            "2021-03-11 12:49:25,613 epoch 9 - iter 32/161 - loss 0.74127025 - samples/sec: 89.28 - lr: 0.100000\n",
            "2021-03-11 12:49:31,413 epoch 9 - iter 48/161 - loss 0.74758157 - samples/sec: 88.31 - lr: 0.100000\n",
            "2021-03-11 12:49:37,154 epoch 9 - iter 64/161 - loss 0.74433655 - samples/sec: 89.22 - lr: 0.100000\n",
            "2021-03-11 12:49:42,983 epoch 9 - iter 80/161 - loss 0.74368926 - samples/sec: 87.87 - lr: 0.100000\n",
            "2021-03-11 12:49:48,820 epoch 9 - iter 96/161 - loss 0.74834082 - samples/sec: 87.75 - lr: 0.100000\n",
            "2021-03-11 12:49:54,709 epoch 9 - iter 112/161 - loss 0.77266969 - samples/sec: 86.99 - lr: 0.100000\n",
            "2021-03-11 12:50:00,560 epoch 9 - iter 128/161 - loss 0.79829952 - samples/sec: 87.54 - lr: 0.100000\n",
            "2021-03-11 12:50:06,546 epoch 9 - iter 144/161 - loss 0.80018679 - samples/sec: 85.57 - lr: 0.100000\n",
            "2021-03-11 12:50:12,450 epoch 9 - iter 160/161 - loss 0.79650180 - samples/sec: 86.76 - lr: 0.100000\n",
            "2021-03-11 12:50:12,594 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:50:12,595 EPOCH 9 done: loss 0.7999 - lr 0.1000000\n",
            "2021-03-11 12:50:15,141 DEV : loss 0.6810588836669922 - score 0.9146\n",
            "2021-03-11 12:50:15,180 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-03-11 12:51:43,536 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:51:52,295 epoch 10 - iter 16/161 - loss 0.76475002 - samples/sec: 88.21 - lr: 0.100000\n",
            "2021-03-11 12:51:58,090 epoch 10 - iter 32/161 - loss 0.75351258 - samples/sec: 88.39 - lr: 0.100000\n",
            "2021-03-11 12:52:03,858 epoch 10 - iter 48/161 - loss 0.74511079 - samples/sec: 88.80 - lr: 0.100000\n",
            "2021-03-11 12:52:09,652 epoch 10 - iter 64/161 - loss 0.75541654 - samples/sec: 88.42 - lr: 0.100000\n",
            "2021-03-11 12:52:15,453 epoch 10 - iter 80/161 - loss 0.74121974 - samples/sec: 88.29 - lr: 0.100000\n",
            "2021-03-11 12:52:21,277 epoch 10 - iter 96/161 - loss 0.74426431 - samples/sec: 87.95 - lr: 0.100000\n",
            "2021-03-11 12:52:27,250 epoch 10 - iter 112/161 - loss 0.75323173 - samples/sec: 85.75 - lr: 0.100000\n",
            "2021-03-11 12:52:33,080 epoch 10 - iter 128/161 - loss 0.75915346 - samples/sec: 87.86 - lr: 0.100000\n",
            "2021-03-11 12:52:39,076 epoch 10 - iter 144/161 - loss 0.75967685 - samples/sec: 85.42 - lr: 0.100000\n",
            "2021-03-11 12:52:44,915 epoch 10 - iter 160/161 - loss 0.75211019 - samples/sec: 87.73 - lr: 0.100000\n",
            "2021-03-11 12:52:45,056 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:52:45,057 EPOCH 10 done: loss 0.7507 - lr 0.1000000\n",
            "2021-03-11 12:52:47,590 DEV : loss 0.6823708415031433 - score 0.9109\n",
            "2021-03-11 12:52:47,629 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 12:52:47,631 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:52:53,440 epoch 11 - iter 16/161 - loss 0.65629457 - samples/sec: 88.20 - lr: 0.100000\n",
            "2021-03-11 12:52:59,142 epoch 11 - iter 32/161 - loss 0.68606065 - samples/sec: 89.84 - lr: 0.100000\n",
            "2021-03-11 12:53:04,934 epoch 11 - iter 48/161 - loss 0.67974381 - samples/sec: 88.44 - lr: 0.100000\n",
            "2021-03-11 12:53:10,781 epoch 11 - iter 64/161 - loss 0.72415839 - samples/sec: 87.60 - lr: 0.100000\n",
            "2021-03-11 12:53:16,553 epoch 11 - iter 80/161 - loss 0.74324410 - samples/sec: 88.73 - lr: 0.100000\n",
            "2021-03-11 12:53:22,367 epoch 11 - iter 96/161 - loss 0.74676332 - samples/sec: 88.11 - lr: 0.100000\n",
            "2021-03-11 12:53:28,100 epoch 11 - iter 112/161 - loss 0.74989924 - samples/sec: 89.34 - lr: 0.100000\n",
            "2021-03-11 12:53:33,949 epoch 11 - iter 128/161 - loss 0.75099522 - samples/sec: 87.58 - lr: 0.100000\n",
            "2021-03-11 12:53:39,914 epoch 11 - iter 144/161 - loss 0.75840146 - samples/sec: 85.86 - lr: 0.100000\n",
            "2021-03-11 12:53:45,818 epoch 11 - iter 160/161 - loss 0.76051437 - samples/sec: 86.76 - lr: 0.100000\n",
            "2021-03-11 12:53:45,958 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:53:45,958 EPOCH 11 done: loss 0.7618 - lr 0.1000000\n",
            "2021-03-11 12:53:48,473 DEV : loss 0.8654462099075317 - score 0.8866\n",
            "2021-03-11 12:53:48,514 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 12:53:48,514 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:53:54,473 epoch 12 - iter 16/161 - loss 0.86887040 - samples/sec: 85.99 - lr: 0.100000\n",
            "2021-03-11 12:54:00,310 epoch 12 - iter 32/161 - loss 0.83821602 - samples/sec: 87.75 - lr: 0.100000\n",
            "2021-03-11 12:54:06,084 epoch 12 - iter 48/161 - loss 0.78939245 - samples/sec: 88.72 - lr: 0.100000\n",
            "2021-03-11 12:54:12,094 epoch 12 - iter 64/161 - loss 0.78889696 - samples/sec: 85.22 - lr: 0.100000\n",
            "2021-03-11 12:54:18,233 epoch 12 - iter 80/161 - loss 0.76977676 - samples/sec: 83.43 - lr: 0.100000\n",
            "2021-03-11 12:54:24,070 epoch 12 - iter 96/161 - loss 0.76003570 - samples/sec: 87.75 - lr: 0.100000\n",
            "2021-03-11 12:54:29,840 epoch 12 - iter 112/161 - loss 0.76023306 - samples/sec: 88.77 - lr: 0.100000\n",
            "2021-03-11 12:54:35,681 epoch 12 - iter 128/161 - loss 0.75952698 - samples/sec: 87.69 - lr: 0.100000\n",
            "2021-03-11 12:54:41,582 epoch 12 - iter 144/161 - loss 0.75185490 - samples/sec: 86.80 - lr: 0.100000\n",
            "2021-03-11 12:54:47,367 epoch 12 - iter 160/161 - loss 0.73641783 - samples/sec: 88.54 - lr: 0.100000\n",
            "2021-03-11 12:54:47,485 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:54:47,486 EPOCH 12 done: loss 0.7329 - lr 0.1000000\n",
            "2021-03-11 12:54:50,008 DEV : loss 0.6553251147270203 - score 0.9197\n",
            "2021-03-11 12:54:50,049 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-03-11 12:56:19,059 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:56:25,079 epoch 13 - iter 16/161 - loss 0.68692331 - samples/sec: 85.13 - lr: 0.100000\n",
            "2021-03-11 12:56:30,824 epoch 13 - iter 32/161 - loss 0.68099860 - samples/sec: 89.16 - lr: 0.100000\n",
            "2021-03-11 12:56:36,775 epoch 13 - iter 48/161 - loss 0.67607271 - samples/sec: 86.07 - lr: 0.100000\n",
            "2021-03-11 12:56:42,595 epoch 13 - iter 64/161 - loss 0.68442267 - samples/sec: 88.02 - lr: 0.100000\n",
            "2021-03-11 12:56:48,353 epoch 13 - iter 80/161 - loss 0.68445481 - samples/sec: 88.95 - lr: 0.100000\n",
            "2021-03-11 12:56:54,218 epoch 13 - iter 96/161 - loss 0.69258459 - samples/sec: 87.34 - lr: 0.100000\n",
            "2021-03-11 12:56:59,961 epoch 13 - iter 112/161 - loss 0.69239185 - samples/sec: 89.20 - lr: 0.100000\n",
            "2021-03-11 12:57:05,791 epoch 13 - iter 128/161 - loss 0.69192459 - samples/sec: 87.85 - lr: 0.100000\n",
            "2021-03-11 12:57:11,585 epoch 13 - iter 144/161 - loss 0.69108526 - samples/sec: 88.40 - lr: 0.100000\n",
            "2021-03-11 12:57:17,473 epoch 13 - iter 160/161 - loss 0.69708486 - samples/sec: 87.00 - lr: 0.100000\n",
            "2021-03-11 12:57:17,599 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:57:17,600 EPOCH 13 done: loss 0.6947 - lr 0.1000000\n",
            "2021-03-11 12:57:20,113 DEV : loss 0.7124163508415222 - score 0.9121\n",
            "2021-03-11 12:57:20,150 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 12:57:20,152 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:57:25,965 epoch 14 - iter 16/161 - loss 0.71958020 - samples/sec: 88.15 - lr: 0.100000\n",
            "2021-03-11 12:57:31,872 epoch 14 - iter 32/161 - loss 0.68983915 - samples/sec: 86.72 - lr: 0.100000\n",
            "2021-03-11 12:57:37,876 epoch 14 - iter 48/161 - loss 0.70509137 - samples/sec: 85.31 - lr: 0.100000\n",
            "2021-03-11 12:57:43,496 epoch 14 - iter 64/161 - loss 0.69765228 - samples/sec: 91.13 - lr: 0.100000\n",
            "2021-03-11 12:57:49,390 epoch 14 - iter 80/161 - loss 0.68734583 - samples/sec: 86.92 - lr: 0.100000\n",
            "2021-03-11 12:57:55,349 epoch 14 - iter 96/161 - loss 0.69849993 - samples/sec: 85.95 - lr: 0.100000\n",
            "2021-03-11 12:58:01,142 epoch 14 - iter 112/161 - loss 0.69496531 - samples/sec: 88.43 - lr: 0.100000\n",
            "2021-03-11 12:58:06,914 epoch 14 - iter 128/161 - loss 0.69470862 - samples/sec: 88.74 - lr: 0.100000\n",
            "2021-03-11 12:58:12,767 epoch 14 - iter 144/161 - loss 0.69036660 - samples/sec: 87.53 - lr: 0.100000\n",
            "2021-03-11 12:58:18,480 epoch 14 - iter 160/161 - loss 0.69203130 - samples/sec: 89.65 - lr: 0.100000\n",
            "2021-03-11 12:58:18,597 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:58:18,598 EPOCH 14 done: loss 0.6953 - lr 0.1000000\n",
            "2021-03-11 12:58:21,094 DEV : loss 0.6431124806404114 - score 0.9143\n",
            "2021-03-11 12:58:21,133 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 12:58:21,135 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:58:27,034 epoch 15 - iter 16/161 - loss 0.80105214 - samples/sec: 86.86 - lr: 0.100000\n",
            "2021-03-11 12:58:32,851 epoch 15 - iter 32/161 - loss 0.74503529 - samples/sec: 88.05 - lr: 0.100000\n",
            "2021-03-11 12:58:38,862 epoch 15 - iter 48/161 - loss 0.71924930 - samples/sec: 85.21 - lr: 0.100000\n",
            "2021-03-11 12:58:44,742 epoch 15 - iter 64/161 - loss 0.73121926 - samples/sec: 87.11 - lr: 0.100000\n",
            "2021-03-11 12:58:50,595 epoch 15 - iter 80/161 - loss 0.72472800 - samples/sec: 87.55 - lr: 0.100000\n",
            "2021-03-11 12:58:56,356 epoch 15 - iter 96/161 - loss 0.71761888 - samples/sec: 88.90 - lr: 0.100000\n",
            "2021-03-11 12:59:02,006 epoch 15 - iter 112/161 - loss 0.69926414 - samples/sec: 90.66 - lr: 0.100000\n",
            "2021-03-11 12:59:07,855 epoch 15 - iter 128/161 - loss 0.69191215 - samples/sec: 87.57 - lr: 0.100000\n",
            "2021-03-11 12:59:13,596 epoch 15 - iter 144/161 - loss 0.68575724 - samples/sec: 89.23 - lr: 0.100000\n",
            "2021-03-11 12:59:19,739 epoch 15 - iter 160/161 - loss 0.68611068 - samples/sec: 83.38 - lr: 0.100000\n",
            "2021-03-11 12:59:19,864 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:59:19,865 EPOCH 15 done: loss 0.6834 - lr 0.1000000\n",
            "2021-03-11 12:59:22,419 DEV : loss 0.6884129047393799 - score 0.9112\n",
            "2021-03-11 12:59:22,458 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 12:59:22,459 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 12:59:28,273 epoch 16 - iter 16/161 - loss 0.66852420 - samples/sec: 88.12 - lr: 0.100000\n",
            "2021-03-11 12:59:34,034 epoch 16 - iter 32/161 - loss 0.72508701 - samples/sec: 88.91 - lr: 0.100000\n",
            "2021-03-11 12:59:39,861 epoch 16 - iter 48/161 - loss 0.70347240 - samples/sec: 87.90 - lr: 0.100000\n",
            "2021-03-11 12:59:45,646 epoch 16 - iter 64/161 - loss 0.70599306 - samples/sec: 88.54 - lr: 0.100000\n",
            "2021-03-11 12:59:51,489 epoch 16 - iter 80/161 - loss 0.70518397 - samples/sec: 87.65 - lr: 0.100000\n",
            "2021-03-11 12:59:57,349 epoch 16 - iter 96/161 - loss 0.68243326 - samples/sec: 87.41 - lr: 0.100000\n",
            "2021-03-11 13:00:03,158 epoch 16 - iter 112/161 - loss 0.67959623 - samples/sec: 88.18 - lr: 0.100000\n",
            "2021-03-11 13:00:08,829 epoch 16 - iter 128/161 - loss 0.66833079 - samples/sec: 90.31 - lr: 0.100000\n",
            "2021-03-11 13:00:14,726 epoch 16 - iter 144/161 - loss 0.67030639 - samples/sec: 86.87 - lr: 0.100000\n",
            "2021-03-11 13:00:20,604 epoch 16 - iter 160/161 - loss 0.66544459 - samples/sec: 87.14 - lr: 0.100000\n",
            "2021-03-11 13:00:20,728 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:00:20,729 EPOCH 16 done: loss 0.6648 - lr 0.1000000\n",
            "2021-03-11 13:00:23,242 DEV : loss 0.7235537767410278 - score 0.9118\n",
            "Epoch    16: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2021-03-11 13:00:23,282 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:00:23,283 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:00:29,173 epoch 17 - iter 16/161 - loss 0.50708502 - samples/sec: 86.99 - lr: 0.050000\n",
            "2021-03-11 13:00:34,936 epoch 17 - iter 32/161 - loss 0.57335808 - samples/sec: 88.87 - lr: 0.050000\n",
            "2021-03-11 13:00:40,792 epoch 17 - iter 48/161 - loss 0.57896764 - samples/sec: 87.47 - lr: 0.050000\n",
            "2021-03-11 13:00:46,818 epoch 17 - iter 64/161 - loss 0.56992358 - samples/sec: 85.01 - lr: 0.050000\n",
            "2021-03-11 13:00:52,734 epoch 17 - iter 80/161 - loss 0.57387858 - samples/sec: 86.58 - lr: 0.050000\n",
            "2021-03-11 13:00:58,516 epoch 17 - iter 96/161 - loss 0.57269062 - samples/sec: 88.59 - lr: 0.050000\n",
            "2021-03-11 13:01:04,297 epoch 17 - iter 112/161 - loss 0.56953694 - samples/sec: 88.61 - lr: 0.050000\n",
            "2021-03-11 13:01:10,106 epoch 17 - iter 128/161 - loss 0.57767520 - samples/sec: 88.17 - lr: 0.050000\n",
            "2021-03-11 13:01:15,872 epoch 17 - iter 144/161 - loss 0.59406014 - samples/sec: 88.83 - lr: 0.050000\n",
            "2021-03-11 13:01:21,614 epoch 17 - iter 160/161 - loss 0.59403161 - samples/sec: 89.20 - lr: 0.050000\n",
            "2021-03-11 13:01:21,730 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:01:21,731 EPOCH 17 done: loss 0.5945 - lr 0.0500000\n",
            "2021-03-11 13:01:24,241 DEV : loss 0.6823051571846008 - score 0.91\n",
            "2021-03-11 13:01:24,282 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:01:24,283 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:01:30,161 epoch 18 - iter 16/161 - loss 0.59918579 - samples/sec: 87.20 - lr: 0.050000\n",
            "2021-03-11 13:01:37,300 epoch 18 - iter 32/161 - loss 0.56519913 - samples/sec: 71.75 - lr: 0.050000\n",
            "2021-03-11 13:01:43,015 epoch 18 - iter 48/161 - loss 0.56438877 - samples/sec: 89.62 - lr: 0.050000\n",
            "2021-03-11 13:01:48,852 epoch 18 - iter 64/161 - loss 0.55841076 - samples/sec: 87.75 - lr: 0.050000\n",
            "2021-03-11 13:01:54,605 epoch 18 - iter 80/161 - loss 0.54989890 - samples/sec: 89.04 - lr: 0.050000\n",
            "2021-03-11 13:02:00,264 epoch 18 - iter 96/161 - loss 0.55587559 - samples/sec: 90.51 - lr: 0.050000\n",
            "2021-03-11 13:02:05,982 epoch 18 - iter 112/161 - loss 0.56355149 - samples/sec: 89.59 - lr: 0.050000\n",
            "2021-03-11 13:02:11,933 epoch 18 - iter 128/161 - loss 0.56976834 - samples/sec: 86.07 - lr: 0.050000\n",
            "2021-03-11 13:02:17,708 epoch 18 - iter 144/161 - loss 0.57201118 - samples/sec: 88.69 - lr: 0.050000\n",
            "2021-03-11 13:02:23,757 epoch 18 - iter 160/161 - loss 0.58514407 - samples/sec: 84.67 - lr: 0.050000\n",
            "2021-03-11 13:02:23,888 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:02:23,889 EPOCH 18 done: loss 0.5818 - lr 0.0500000\n",
            "2021-03-11 13:02:26,476 DEV : loss 0.6383929252624512 - score 0.9164\n",
            "2021-03-11 13:02:26,516 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:02:26,517 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:02:32,429 epoch 19 - iter 16/161 - loss 0.55602025 - samples/sec: 86.67 - lr: 0.050000\n",
            "2021-03-11 13:02:38,474 epoch 19 - iter 32/161 - loss 0.53742881 - samples/sec: 84.73 - lr: 0.050000\n",
            "2021-03-11 13:02:44,274 epoch 19 - iter 48/161 - loss 0.56540934 - samples/sec: 88.32 - lr: 0.050000\n",
            "2021-03-11 13:02:50,106 epoch 19 - iter 64/161 - loss 0.55536351 - samples/sec: 87.84 - lr: 0.050000\n",
            "2021-03-11 13:02:56,179 epoch 19 - iter 80/161 - loss 0.56783382 - samples/sec: 84.34 - lr: 0.050000\n",
            "2021-03-11 13:03:02,112 epoch 19 - iter 96/161 - loss 0.55735328 - samples/sec: 86.33 - lr: 0.050000\n",
            "2021-03-11 13:03:08,112 epoch 19 - iter 112/161 - loss 0.56022155 - samples/sec: 85.37 - lr: 0.050000\n",
            "2021-03-11 13:03:14,350 epoch 19 - iter 128/161 - loss 0.57174611 - samples/sec: 82.12 - lr: 0.050000\n",
            "2021-03-11 13:03:20,484 epoch 19 - iter 144/161 - loss 0.57086301 - samples/sec: 83.49 - lr: 0.050000\n",
            "2021-03-11 13:03:26,474 epoch 19 - iter 160/161 - loss 0.56169289 - samples/sec: 85.52 - lr: 0.050000\n",
            "2021-03-11 13:03:26,600 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:03:26,601 EPOCH 19 done: loss 0.5590 - lr 0.0500000\n",
            "2021-03-11 13:03:29,222 DEV : loss 0.6734601855278015 - score 0.9161\n",
            "2021-03-11 13:03:29,264 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:03:29,265 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:03:35,301 epoch 20 - iter 16/161 - loss 0.61902736 - samples/sec: 84.91 - lr: 0.050000\n",
            "2021-03-11 13:03:41,348 epoch 20 - iter 32/161 - loss 0.60468037 - samples/sec: 84.70 - lr: 0.050000\n",
            "2021-03-11 13:03:47,161 epoch 20 - iter 48/161 - loss 0.56689062 - samples/sec: 88.12 - lr: 0.050000\n",
            "2021-03-11 13:03:53,090 epoch 20 - iter 64/161 - loss 0.54951031 - samples/sec: 86.40 - lr: 0.050000\n",
            "2021-03-11 13:03:59,045 epoch 20 - iter 80/161 - loss 0.53750353 - samples/sec: 86.02 - lr: 0.050000\n",
            "2021-03-11 13:04:05,048 epoch 20 - iter 96/161 - loss 0.55945070 - samples/sec: 85.32 - lr: 0.050000\n",
            "2021-03-11 13:04:10,908 epoch 20 - iter 112/161 - loss 0.56225043 - samples/sec: 87.41 - lr: 0.050000\n",
            "2021-03-11 13:04:16,668 epoch 20 - iter 128/161 - loss 0.56283902 - samples/sec: 88.92 - lr: 0.050000\n",
            "2021-03-11 13:04:22,573 epoch 20 - iter 144/161 - loss 0.54881221 - samples/sec: 86.74 - lr: 0.050000\n",
            "2021-03-11 13:04:28,519 epoch 20 - iter 160/161 - loss 0.54991371 - samples/sec: 86.14 - lr: 0.050000\n",
            "2021-03-11 13:04:28,619 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:04:28,619 EPOCH 20 done: loss 0.5479 - lr 0.0500000\n",
            "2021-03-11 13:04:31,241 DEV : loss 0.6518872976303101 - score 0.9238\n",
            "2021-03-11 13:04:31,280 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2021-03-11 13:06:01,493 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:06:07,505 epoch 21 - iter 16/161 - loss 0.58980053 - samples/sec: 85.47 - lr: 0.050000\n",
            "2021-03-11 13:06:13,523 epoch 21 - iter 32/161 - loss 0.54339125 - samples/sec: 85.11 - lr: 0.050000\n",
            "2021-03-11 13:06:19,352 epoch 21 - iter 48/161 - loss 0.55349026 - samples/sec: 87.88 - lr: 0.050000\n",
            "2021-03-11 13:06:25,305 epoch 21 - iter 64/161 - loss 0.51990075 - samples/sec: 86.05 - lr: 0.050000\n",
            "2021-03-11 13:06:31,355 epoch 21 - iter 80/161 - loss 0.51298602 - samples/sec: 84.66 - lr: 0.050000\n",
            "2021-03-11 13:06:37,417 epoch 21 - iter 96/161 - loss 0.52679262 - samples/sec: 84.50 - lr: 0.050000\n",
            "2021-03-11 13:06:43,333 epoch 21 - iter 112/161 - loss 0.52894209 - samples/sec: 86.58 - lr: 0.050000\n",
            "2021-03-11 13:06:49,376 epoch 21 - iter 128/161 - loss 0.53665757 - samples/sec: 84.76 - lr: 0.050000\n",
            "2021-03-11 13:06:55,440 epoch 21 - iter 144/161 - loss 0.54611322 - samples/sec: 84.47 - lr: 0.050000\n",
            "2021-03-11 13:07:01,329 epoch 21 - iter 160/161 - loss 0.54020885 - samples/sec: 86.97 - lr: 0.050000\n",
            "2021-03-11 13:07:01,461 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:07:01,462 EPOCH 21 done: loss 0.5400 - lr 0.0500000\n",
            "2021-03-11 13:07:04,034 DEV : loss 0.6605895161628723 - score 0.9159\n",
            "2021-03-11 13:07:04,073 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:07:04,075 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:07:09,939 epoch 22 - iter 16/161 - loss 0.47434731 - samples/sec: 87.37 - lr: 0.050000\n",
            "2021-03-11 13:07:16,113 epoch 22 - iter 32/161 - loss 0.49179067 - samples/sec: 82.96 - lr: 0.050000\n",
            "2021-03-11 13:07:22,012 epoch 22 - iter 48/161 - loss 0.52339100 - samples/sec: 86.83 - lr: 0.050000\n",
            "2021-03-11 13:07:28,038 epoch 22 - iter 64/161 - loss 0.52928280 - samples/sec: 85.00 - lr: 0.050000\n",
            "2021-03-11 13:07:34,065 epoch 22 - iter 80/161 - loss 0.53878669 - samples/sec: 84.99 - lr: 0.050000\n",
            "2021-03-11 13:07:39,994 epoch 22 - iter 96/161 - loss 0.53270747 - samples/sec: 86.39 - lr: 0.050000\n",
            "2021-03-11 13:07:45,997 epoch 22 - iter 112/161 - loss 0.52298071 - samples/sec: 85.32 - lr: 0.050000\n",
            "2021-03-11 13:07:51,705 epoch 22 - iter 128/161 - loss 0.52313870 - samples/sec: 89.74 - lr: 0.050000\n",
            "2021-03-11 13:07:57,694 epoch 22 - iter 144/161 - loss 0.53133131 - samples/sec: 85.52 - lr: 0.050000\n",
            "2021-03-11 13:08:03,596 epoch 22 - iter 160/161 - loss 0.52794525 - samples/sec: 86.78 - lr: 0.050000\n",
            "2021-03-11 13:08:03,730 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:08:03,731 EPOCH 22 done: loss 0.5281 - lr 0.0500000\n",
            "2021-03-11 13:08:06,288 DEV : loss 0.6677911877632141 - score 0.9157\n",
            "2021-03-11 13:08:06,328 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:08:06,329 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:08:12,254 epoch 23 - iter 16/161 - loss 0.54396998 - samples/sec: 86.48 - lr: 0.050000\n",
            "2021-03-11 13:08:18,103 epoch 23 - iter 32/161 - loss 0.55730796 - samples/sec: 87.57 - lr: 0.050000\n",
            "2021-03-11 13:08:23,956 epoch 23 - iter 48/161 - loss 0.53523340 - samples/sec: 87.52 - lr: 0.050000\n",
            "2021-03-11 13:08:30,083 epoch 23 - iter 64/161 - loss 0.54028261 - samples/sec: 83.59 - lr: 0.050000\n",
            "2021-03-11 13:08:36,020 epoch 23 - iter 80/161 - loss 0.52878332 - samples/sec: 86.28 - lr: 0.050000\n",
            "2021-03-11 13:08:41,808 epoch 23 - iter 96/161 - loss 0.53787381 - samples/sec: 88.49 - lr: 0.050000\n",
            "2021-03-11 13:08:47,725 epoch 23 - iter 112/161 - loss 0.52957161 - samples/sec: 86.58 - lr: 0.050000\n",
            "2021-03-11 13:08:53,519 epoch 23 - iter 128/161 - loss 0.52570361 - samples/sec: 88.40 - lr: 0.050000\n",
            "2021-03-11 13:08:59,454 epoch 23 - iter 144/161 - loss 0.52448122 - samples/sec: 86.30 - lr: 0.050000\n",
            "2021-03-11 13:09:05,316 epoch 23 - iter 160/161 - loss 0.52690529 - samples/sec: 87.38 - lr: 0.050000\n",
            "2021-03-11 13:09:05,470 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:09:05,472 EPOCH 23 done: loss 0.5275 - lr 0.0500000\n",
            "2021-03-11 13:09:08,040 DEV : loss 0.6627979874610901 - score 0.9146\n",
            "2021-03-11 13:09:08,080 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:09:08,082 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:09:13,923 epoch 24 - iter 16/161 - loss 0.49836998 - samples/sec: 87.71 - lr: 0.050000\n",
            "2021-03-11 13:09:19,865 epoch 24 - iter 32/161 - loss 0.51637109 - samples/sec: 86.20 - lr: 0.050000\n",
            "2021-03-11 13:09:25,733 epoch 24 - iter 48/161 - loss 0.50155341 - samples/sec: 87.29 - lr: 0.050000\n",
            "2021-03-11 13:09:31,739 epoch 24 - iter 64/161 - loss 0.50514949 - samples/sec: 85.29 - lr: 0.050000\n",
            "2021-03-11 13:09:37,755 epoch 24 - iter 80/161 - loss 0.51401954 - samples/sec: 85.14 - lr: 0.050000\n",
            "2021-03-11 13:09:43,628 epoch 24 - iter 96/161 - loss 0.50845669 - samples/sec: 87.23 - lr: 0.050000\n",
            "2021-03-11 13:09:49,575 epoch 24 - iter 112/161 - loss 0.50878617 - samples/sec: 86.12 - lr: 0.050000\n",
            "2021-03-11 13:09:55,443 epoch 24 - iter 128/161 - loss 0.52058230 - samples/sec: 87.29 - lr: 0.050000\n",
            "2021-03-11 13:10:01,284 epoch 24 - iter 144/161 - loss 0.52149545 - samples/sec: 87.70 - lr: 0.050000\n",
            "2021-03-11 13:10:07,244 epoch 24 - iter 160/161 - loss 0.51160389 - samples/sec: 85.95 - lr: 0.050000\n",
            "2021-03-11 13:10:07,381 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:10:07,382 EPOCH 24 done: loss 0.5217 - lr 0.0500000\n",
            "2021-03-11 13:10:09,959 DEV : loss 0.6384405493736267 - score 0.9161\n",
            "Epoch    24: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2021-03-11 13:10:10,000 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:10:10,001 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:10:15,935 epoch 25 - iter 16/161 - loss 0.44828453 - samples/sec: 86.36 - lr: 0.025000\n",
            "2021-03-11 13:10:21,827 epoch 25 - iter 32/161 - loss 0.46960720 - samples/sec: 86.93 - lr: 0.025000\n",
            "2021-03-11 13:10:27,683 epoch 25 - iter 48/161 - loss 0.47195124 - samples/sec: 87.48 - lr: 0.025000\n",
            "2021-03-11 13:10:33,566 epoch 25 - iter 64/161 - loss 0.49467002 - samples/sec: 87.06 - lr: 0.025000\n",
            "2021-03-11 13:10:39,409 epoch 25 - iter 80/161 - loss 0.50932086 - samples/sec: 87.67 - lr: 0.025000\n",
            "2021-03-11 13:10:45,376 epoch 25 - iter 96/161 - loss 0.50447729 - samples/sec: 85.84 - lr: 0.025000\n",
            "2021-03-11 13:10:51,240 epoch 25 - iter 112/161 - loss 0.50130631 - samples/sec: 87.35 - lr: 0.025000\n",
            "2021-03-11 13:10:57,100 epoch 25 - iter 128/161 - loss 0.51045726 - samples/sec: 87.41 - lr: 0.025000\n",
            "2021-03-11 13:11:03,030 epoch 25 - iter 144/161 - loss 0.50005263 - samples/sec: 86.38 - lr: 0.025000\n",
            "2021-03-11 13:11:08,969 epoch 25 - iter 160/161 - loss 0.49723325 - samples/sec: 86.24 - lr: 0.025000\n",
            "2021-03-11 13:11:09,100 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:11:09,101 EPOCH 25 done: loss 0.4948 - lr 0.0250000\n",
            "2021-03-11 13:11:11,634 DEV : loss 0.6412671804428101 - score 0.9195\n",
            "2021-03-11 13:11:11,673 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:11:11,674 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:11:17,704 epoch 26 - iter 16/161 - loss 0.43466580 - samples/sec: 84.98 - lr: 0.025000\n",
            "2021-03-11 13:11:23,605 epoch 26 - iter 32/161 - loss 0.45659833 - samples/sec: 86.80 - lr: 0.025000\n",
            "2021-03-11 13:11:29,488 epoch 26 - iter 48/161 - loss 0.47548408 - samples/sec: 87.08 - lr: 0.025000\n",
            "2021-03-11 13:11:35,474 epoch 26 - iter 64/161 - loss 0.47676578 - samples/sec: 85.57 - lr: 0.025000\n",
            "2021-03-11 13:11:41,478 epoch 26 - iter 80/161 - loss 0.46458048 - samples/sec: 85.31 - lr: 0.025000\n",
            "2021-03-11 13:11:47,286 epoch 26 - iter 96/161 - loss 0.45484507 - samples/sec: 88.19 - lr: 0.025000\n",
            "2021-03-11 13:11:53,103 epoch 26 - iter 112/161 - loss 0.45496840 - samples/sec: 88.05 - lr: 0.025000\n",
            "2021-03-11 13:11:58,979 epoch 26 - iter 128/161 - loss 0.45165285 - samples/sec: 87.17 - lr: 0.025000\n",
            "2021-03-11 13:12:04,800 epoch 26 - iter 144/161 - loss 0.45623539 - samples/sec: 87.99 - lr: 0.025000\n",
            "2021-03-11 13:12:10,608 epoch 26 - iter 160/161 - loss 0.45818061 - samples/sec: 88.20 - lr: 0.025000\n",
            "2021-03-11 13:12:10,762 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:12:10,762 EPOCH 26 done: loss 0.4609 - lr 0.0250000\n",
            "2021-03-11 13:12:13,285 DEV : loss 0.6555718779563904 - score 0.9178\n",
            "2021-03-11 13:12:13,326 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:12:13,327 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:12:19,163 epoch 27 - iter 16/161 - loss 0.45031185 - samples/sec: 87.80 - lr: 0.025000\n",
            "2021-03-11 13:12:25,058 epoch 27 - iter 32/161 - loss 0.48817038 - samples/sec: 86.89 - lr: 0.025000\n",
            "2021-03-11 13:12:30,874 epoch 27 - iter 48/161 - loss 0.48188307 - samples/sec: 88.07 - lr: 0.025000\n",
            "2021-03-11 13:12:36,711 epoch 27 - iter 64/161 - loss 0.46538128 - samples/sec: 87.75 - lr: 0.025000\n",
            "2021-03-11 13:12:42,610 epoch 27 - iter 80/161 - loss 0.46581705 - samples/sec: 86.84 - lr: 0.025000\n",
            "2021-03-11 13:12:48,485 epoch 27 - iter 96/161 - loss 0.45867500 - samples/sec: 87.18 - lr: 0.025000\n",
            "2021-03-11 13:12:54,430 epoch 27 - iter 112/161 - loss 0.45905710 - samples/sec: 86.17 - lr: 0.025000\n",
            "2021-03-11 13:13:00,318 epoch 27 - iter 128/161 - loss 0.46531569 - samples/sec: 86.99 - lr: 0.025000\n",
            "2021-03-11 13:13:05,931 epoch 27 - iter 144/161 - loss 0.45980609 - samples/sec: 91.25 - lr: 0.025000\n",
            "2021-03-11 13:13:11,775 epoch 27 - iter 160/161 - loss 0.46454012 - samples/sec: 87.64 - lr: 0.025000\n",
            "2021-03-11 13:13:11,917 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:13:11,918 EPOCH 27 done: loss 0.4636 - lr 0.0250000\n",
            "2021-03-11 13:13:14,440 DEV : loss 0.6319429278373718 - score 0.9232\n",
            "2021-03-11 13:13:14,482 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:13:14,483 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:13:20,319 epoch 28 - iter 16/161 - loss 0.50057754 - samples/sec: 87.79 - lr: 0.025000\n",
            "2021-03-11 13:13:26,101 epoch 28 - iter 32/161 - loss 0.43690612 - samples/sec: 88.59 - lr: 0.025000\n",
            "2021-03-11 13:13:31,863 epoch 28 - iter 48/161 - loss 0.43605214 - samples/sec: 88.91 - lr: 0.025000\n",
            "2021-03-11 13:13:37,647 epoch 28 - iter 64/161 - loss 0.43529095 - samples/sec: 88.55 - lr: 0.025000\n",
            "2021-03-11 13:13:43,567 epoch 28 - iter 80/161 - loss 0.42222585 - samples/sec: 86.52 - lr: 0.025000\n",
            "2021-03-11 13:13:49,351 epoch 28 - iter 96/161 - loss 0.42119835 - samples/sec: 88.56 - lr: 0.025000\n",
            "2021-03-11 13:13:55,411 epoch 28 - iter 112/161 - loss 0.43682279 - samples/sec: 84.52 - lr: 0.025000\n",
            "2021-03-11 13:14:01,345 epoch 28 - iter 128/161 - loss 0.44578955 - samples/sec: 86.32 - lr: 0.025000\n",
            "2021-03-11 13:14:07,125 epoch 28 - iter 144/161 - loss 0.45695714 - samples/sec: 88.61 - lr: 0.025000\n",
            "2021-03-11 13:14:12,949 epoch 28 - iter 160/161 - loss 0.46053544 - samples/sec: 87.95 - lr: 0.025000\n",
            "2021-03-11 13:14:13,084 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:14:13,085 EPOCH 28 done: loss 0.4586 - lr 0.0250000\n",
            "2021-03-11 13:14:15,662 DEV : loss 0.6334441304206848 - score 0.9164\n",
            "Epoch    28: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2021-03-11 13:14:15,701 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:14:15,702 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:14:21,616 epoch 29 - iter 16/161 - loss 0.46266202 - samples/sec: 86.65 - lr: 0.012500\n",
            "2021-03-11 13:14:27,434 epoch 29 - iter 32/161 - loss 0.46515276 - samples/sec: 88.03 - lr: 0.012500\n",
            "2021-03-11 13:14:33,340 epoch 29 - iter 48/161 - loss 0.47046404 - samples/sec: 86.72 - lr: 0.012500\n",
            "2021-03-11 13:14:39,179 epoch 29 - iter 64/161 - loss 0.45944336 - samples/sec: 87.73 - lr: 0.012500\n",
            "2021-03-11 13:14:45,019 epoch 29 - iter 80/161 - loss 0.45295564 - samples/sec: 87.71 - lr: 0.012500\n",
            "2021-03-11 13:14:50,855 epoch 29 - iter 96/161 - loss 0.45739384 - samples/sec: 87.77 - lr: 0.012500\n",
            "2021-03-11 13:14:56,673 epoch 29 - iter 112/161 - loss 0.45353315 - samples/sec: 88.03 - lr: 0.012500\n",
            "2021-03-11 13:15:02,515 epoch 29 - iter 128/161 - loss 0.45452790 - samples/sec: 87.67 - lr: 0.012500\n",
            "2021-03-11 13:15:08,333 epoch 29 - iter 144/161 - loss 0.45433231 - samples/sec: 88.05 - lr: 0.012500\n",
            "2021-03-11 13:15:14,337 epoch 29 - iter 160/161 - loss 0.45243673 - samples/sec: 85.31 - lr: 0.012500\n",
            "2021-03-11 13:15:14,470 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:15:14,471 EPOCH 29 done: loss 0.4514 - lr 0.0125000\n",
            "2021-03-11 13:15:17,021 DEV : loss 0.6346413493156433 - score 0.9216\n",
            "2021-03-11 13:15:17,060 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:15:17,061 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:15:23,004 epoch 30 - iter 16/161 - loss 0.45583154 - samples/sec: 86.21 - lr: 0.012500\n",
            "2021-03-11 13:15:28,942 epoch 30 - iter 32/161 - loss 0.46172338 - samples/sec: 86.27 - lr: 0.012500\n",
            "2021-03-11 13:15:34,788 epoch 30 - iter 48/161 - loss 0.43910934 - samples/sec: 87.63 - lr: 0.012500\n",
            "2021-03-11 13:15:40,703 epoch 30 - iter 64/161 - loss 0.43086495 - samples/sec: 86.59 - lr: 0.012500\n",
            "2021-03-11 13:15:46,582 epoch 30 - iter 80/161 - loss 0.43135374 - samples/sec: 87.13 - lr: 0.012500\n",
            "2021-03-11 13:15:52,412 epoch 30 - iter 96/161 - loss 0.41835721 - samples/sec: 87.85 - lr: 0.012500\n",
            "2021-03-11 13:15:58,255 epoch 30 - iter 112/161 - loss 0.42031098 - samples/sec: 87.66 - lr: 0.012500\n",
            "2021-03-11 13:16:03,977 epoch 30 - iter 128/161 - loss 0.43121139 - samples/sec: 89.53 - lr: 0.012500\n",
            "2021-03-11 13:16:09,779 epoch 30 - iter 144/161 - loss 0.43023179 - samples/sec: 88.29 - lr: 0.012500\n",
            "2021-03-11 13:16:15,673 epoch 30 - iter 160/161 - loss 0.42912707 - samples/sec: 86.90 - lr: 0.012500\n",
            "2021-03-11 13:16:15,792 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:16:15,793 EPOCH 30 done: loss 0.4283 - lr 0.0125000\n",
            "2021-03-11 13:16:18,313 DEV : loss 0.6459820866584778 - score 0.9136\n",
            "2021-03-11 13:16:18,351 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:16:18,352 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:16:24,336 epoch 31 - iter 16/161 - loss 0.54268867 - samples/sec: 85.63 - lr: 0.012500\n",
            "2021-03-11 13:16:30,100 epoch 31 - iter 32/161 - loss 0.48533342 - samples/sec: 88.87 - lr: 0.012500\n",
            "2021-03-11 13:16:35,996 epoch 31 - iter 48/161 - loss 0.48193202 - samples/sec: 86.87 - lr: 0.012500\n",
            "2021-03-11 13:16:41,824 epoch 31 - iter 64/161 - loss 0.47680450 - samples/sec: 87.89 - lr: 0.012500\n",
            "2021-03-11 13:16:47,744 epoch 31 - iter 80/161 - loss 0.46427391 - samples/sec: 86.53 - lr: 0.012500\n",
            "2021-03-11 13:16:53,534 epoch 31 - iter 96/161 - loss 0.45217944 - samples/sec: 88.47 - lr: 0.012500\n",
            "2021-03-11 13:16:59,362 epoch 31 - iter 112/161 - loss 0.45400363 - samples/sec: 87.89 - lr: 0.012500\n",
            "2021-03-11 13:17:05,149 epoch 31 - iter 128/161 - loss 0.44818682 - samples/sec: 88.51 - lr: 0.012500\n",
            "2021-03-11 13:17:10,998 epoch 31 - iter 144/161 - loss 0.44889563 - samples/sec: 87.57 - lr: 0.012500\n",
            "2021-03-11 13:17:16,870 epoch 31 - iter 160/161 - loss 0.44473134 - samples/sec: 87.22 - lr: 0.012500\n",
            "2021-03-11 13:17:16,980 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:17:16,981 EPOCH 31 done: loss 0.4445 - lr 0.0125000\n",
            "2021-03-11 13:17:19,544 DEV : loss 0.6542726159095764 - score 0.9146\n",
            "2021-03-11 13:17:19,583 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:17:19,584 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:17:25,458 epoch 32 - iter 16/161 - loss 0.43713416 - samples/sec: 87.24 - lr: 0.012500\n",
            "2021-03-11 13:17:31,325 epoch 32 - iter 32/161 - loss 0.43503607 - samples/sec: 87.29 - lr: 0.012500\n",
            "2021-03-11 13:17:37,098 epoch 32 - iter 48/161 - loss 0.43398511 - samples/sec: 88.73 - lr: 0.012500\n",
            "2021-03-11 13:17:42,888 epoch 32 - iter 64/161 - loss 0.41742141 - samples/sec: 88.47 - lr: 0.012500\n",
            "2021-03-11 13:17:48,669 epoch 32 - iter 80/161 - loss 0.40823438 - samples/sec: 88.60 - lr: 0.012500\n",
            "2021-03-11 13:17:54,694 epoch 32 - iter 96/161 - loss 0.41602811 - samples/sec: 85.02 - lr: 0.012500\n",
            "2021-03-11 13:18:00,487 epoch 32 - iter 112/161 - loss 0.41145803 - samples/sec: 88.42 - lr: 0.012500\n",
            "2021-03-11 13:18:06,349 epoch 32 - iter 128/161 - loss 0.41715724 - samples/sec: 87.38 - lr: 0.012500\n",
            "2021-03-11 13:18:12,369 epoch 32 - iter 144/161 - loss 0.42090973 - samples/sec: 85.08 - lr: 0.012500\n",
            "2021-03-11 13:18:18,387 epoch 32 - iter 160/161 - loss 0.42668774 - samples/sec: 85.12 - lr: 0.012500\n",
            "2021-03-11 13:18:18,529 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:18:18,530 EPOCH 32 done: loss 0.4292 - lr 0.0125000\n",
            "2021-03-11 13:18:21,128 DEV : loss 0.6493107676506042 - score 0.9151\n",
            "Epoch    32: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2021-03-11 13:18:21,168 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:18:21,169 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:18:27,047 epoch 33 - iter 16/161 - loss 0.40931956 - samples/sec: 87.17 - lr: 0.006250\n",
            "2021-03-11 13:18:33,027 epoch 33 - iter 32/161 - loss 0.43824764 - samples/sec: 85.65 - lr: 0.006250\n",
            "2021-03-11 13:18:38,903 epoch 33 - iter 48/161 - loss 0.42936224 - samples/sec: 87.17 - lr: 0.006250\n",
            "2021-03-11 13:18:44,681 epoch 33 - iter 64/161 - loss 0.41426962 - samples/sec: 88.65 - lr: 0.006250\n",
            "2021-03-11 13:18:50,592 epoch 33 - iter 80/161 - loss 0.42016547 - samples/sec: 86.66 - lr: 0.006250\n",
            "2021-03-11 13:18:56,425 epoch 33 - iter 96/161 - loss 0.41678732 - samples/sec: 87.81 - lr: 0.006250\n",
            "2021-03-11 13:19:02,154 epoch 33 - iter 112/161 - loss 0.42033094 - samples/sec: 89.41 - lr: 0.006250\n",
            "2021-03-11 13:19:08,121 epoch 33 - iter 128/161 - loss 0.42917407 - samples/sec: 85.84 - lr: 0.006250\n",
            "2021-03-11 13:19:13,961 epoch 33 - iter 144/161 - loss 0.42924540 - samples/sec: 87.71 - lr: 0.006250\n",
            "2021-03-11 13:19:19,740 epoch 33 - iter 160/161 - loss 0.42759707 - samples/sec: 88.64 - lr: 0.006250\n",
            "2021-03-11 13:19:19,861 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:19:19,861 EPOCH 33 done: loss 0.4325 - lr 0.0062500\n",
            "2021-03-11 13:19:22,405 DEV : loss 0.6523300409317017 - score 0.9156\n",
            "2021-03-11 13:19:22,444 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:19:22,445 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:19:28,343 epoch 34 - iter 16/161 - loss 0.45613033 - samples/sec: 86.85 - lr: 0.006250\n",
            "2021-03-11 13:19:34,403 epoch 34 - iter 32/161 - loss 0.42636065 - samples/sec: 84.54 - lr: 0.006250\n",
            "2021-03-11 13:19:40,249 epoch 34 - iter 48/161 - loss 0.41442224 - samples/sec: 87.61 - lr: 0.006250\n",
            "2021-03-11 13:19:46,099 epoch 34 - iter 64/161 - loss 0.41732395 - samples/sec: 87.56 - lr: 0.006250\n",
            "2021-03-11 13:19:51,921 epoch 34 - iter 80/161 - loss 0.41927134 - samples/sec: 87.98 - lr: 0.006250\n",
            "2021-03-11 13:19:57,970 epoch 34 - iter 96/161 - loss 0.42882924 - samples/sec: 84.68 - lr: 0.006250\n",
            "2021-03-11 13:20:03,958 epoch 34 - iter 112/161 - loss 0.42530779 - samples/sec: 85.54 - lr: 0.006250\n",
            "2021-03-11 13:20:09,870 epoch 34 - iter 128/161 - loss 0.42874925 - samples/sec: 86.63 - lr: 0.006250\n",
            "2021-03-11 13:20:15,655 epoch 34 - iter 144/161 - loss 0.42852815 - samples/sec: 88.54 - lr: 0.006250\n",
            "2021-03-11 13:20:21,444 epoch 34 - iter 160/161 - loss 0.43325334 - samples/sec: 88.49 - lr: 0.006250\n",
            "2021-03-11 13:20:21,578 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:20:21,579 EPOCH 34 done: loss 0.4321 - lr 0.0062500\n",
            "2021-03-11 13:20:24,111 DEV : loss 0.6480966806411743 - score 0.9173\n",
            "2021-03-11 13:20:24,150 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:20:24,152 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:20:29,932 epoch 35 - iter 16/161 - loss 0.44991633 - samples/sec: 88.64 - lr: 0.006250\n",
            "2021-03-11 13:20:35,881 epoch 35 - iter 32/161 - loss 0.43638042 - samples/sec: 86.10 - lr: 0.006250\n",
            "2021-03-11 13:20:41,766 epoch 35 - iter 48/161 - loss 0.40577184 - samples/sec: 87.03 - lr: 0.006250\n",
            "2021-03-11 13:20:48,881 epoch 35 - iter 64/161 - loss 0.41005844 - samples/sec: 71.99 - lr: 0.006250\n",
            "2021-03-11 13:20:54,803 epoch 35 - iter 80/161 - loss 0.41494810 - samples/sec: 86.48 - lr: 0.006250\n",
            "2021-03-11 13:21:00,527 epoch 35 - iter 96/161 - loss 0.42371952 - samples/sec: 89.50 - lr: 0.006250\n",
            "2021-03-11 13:21:06,468 epoch 35 - iter 112/161 - loss 0.42389406 - samples/sec: 86.21 - lr: 0.006250\n",
            "2021-03-11 13:21:12,368 epoch 35 - iter 128/161 - loss 0.42796160 - samples/sec: 86.81 - lr: 0.006250\n",
            "2021-03-11 13:21:18,164 epoch 35 - iter 144/161 - loss 0.42145854 - samples/sec: 88.38 - lr: 0.006250\n",
            "2021-03-11 13:21:24,004 epoch 35 - iter 160/161 - loss 0.41945939 - samples/sec: 87.71 - lr: 0.006250\n",
            "2021-03-11 13:21:24,125 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:21:24,126 EPOCH 35 done: loss 0.4220 - lr 0.0062500\n",
            "2021-03-11 13:21:26,663 DEV : loss 0.652953565120697 - score 0.9152\n",
            "2021-03-11 13:21:26,702 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:21:26,703 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:21:32,440 epoch 36 - iter 16/161 - loss 0.40231323 - samples/sec: 89.31 - lr: 0.006250\n",
            "2021-03-11 13:21:38,286 epoch 36 - iter 32/161 - loss 0.39093990 - samples/sec: 87.63 - lr: 0.006250\n",
            "2021-03-11 13:21:44,135 epoch 36 - iter 48/161 - loss 0.41217100 - samples/sec: 87.57 - lr: 0.006250\n",
            "2021-03-11 13:21:50,062 epoch 36 - iter 64/161 - loss 0.41361925 - samples/sec: 86.42 - lr: 0.006250\n",
            "2021-03-11 13:21:55,802 epoch 36 - iter 80/161 - loss 0.41303958 - samples/sec: 89.24 - lr: 0.006250\n",
            "2021-03-11 13:22:01,804 epoch 36 - iter 96/161 - loss 0.41709647 - samples/sec: 85.34 - lr: 0.006250\n",
            "2021-03-11 13:22:07,738 epoch 36 - iter 112/161 - loss 0.42169930 - samples/sec: 86.31 - lr: 0.006250\n",
            "2021-03-11 13:22:13,614 epoch 36 - iter 128/161 - loss 0.42300158 - samples/sec: 87.18 - lr: 0.006250\n",
            "2021-03-11 13:22:19,569 epoch 36 - iter 144/161 - loss 0.42335824 - samples/sec: 86.01 - lr: 0.006250\n",
            "2021-03-11 13:22:25,378 epoch 36 - iter 160/161 - loss 0.42099265 - samples/sec: 88.18 - lr: 0.006250\n",
            "2021-03-11 13:22:25,482 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:22:25,483 EPOCH 36 done: loss 0.4188 - lr 0.0062500\n",
            "2021-03-11 13:22:28,008 DEV : loss 0.6584500670433044 - score 0.9158\n",
            "Epoch    36: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2021-03-11 13:22:28,047 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:22:28,048 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:22:33,828 epoch 37 - iter 16/161 - loss 0.40507898 - samples/sec: 88.70 - lr: 0.003125\n",
            "2021-03-11 13:22:39,658 epoch 37 - iter 32/161 - loss 0.39978242 - samples/sec: 87.85 - lr: 0.003125\n",
            "2021-03-11 13:22:45,425 epoch 37 - iter 48/161 - loss 0.40658535 - samples/sec: 88.82 - lr: 0.003125\n",
            "2021-03-11 13:22:51,456 epoch 37 - iter 64/161 - loss 0.41404268 - samples/sec: 84.93 - lr: 0.003125\n",
            "2021-03-11 13:22:57,457 epoch 37 - iter 80/161 - loss 0.41022064 - samples/sec: 85.36 - lr: 0.003125\n",
            "2021-03-11 13:23:03,290 epoch 37 - iter 96/161 - loss 0.41587003 - samples/sec: 87.80 - lr: 0.003125\n",
            "2021-03-11 13:23:09,073 epoch 37 - iter 112/161 - loss 0.41472381 - samples/sec: 88.57 - lr: 0.003125\n",
            "2021-03-11 13:23:14,833 epoch 37 - iter 128/161 - loss 0.41029409 - samples/sec: 88.93 - lr: 0.003125\n",
            "2021-03-11 13:23:20,663 epoch 37 - iter 144/161 - loss 0.40780960 - samples/sec: 87.86 - lr: 0.003125\n",
            "2021-03-11 13:23:26,508 epoch 37 - iter 160/161 - loss 0.41000474 - samples/sec: 87.64 - lr: 0.003125\n",
            "2021-03-11 13:23:26,619 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:23:26,620 EPOCH 37 done: loss 0.4085 - lr 0.0031250\n",
            "2021-03-11 13:23:29,158 DEV : loss 0.6537882685661316 - score 0.9129\n",
            "2021-03-11 13:23:29,197 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:23:29,198 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:23:35,044 epoch 38 - iter 16/161 - loss 0.45902175 - samples/sec: 87.64 - lr: 0.003125\n",
            "2021-03-11 13:23:41,052 epoch 38 - iter 32/161 - loss 0.44579208 - samples/sec: 85.26 - lr: 0.003125\n",
            "2021-03-11 13:23:47,022 epoch 38 - iter 48/161 - loss 0.41436769 - samples/sec: 85.79 - lr: 0.003125\n",
            "2021-03-11 13:23:52,805 epoch 38 - iter 64/161 - loss 0.42226511 - samples/sec: 88.58 - lr: 0.003125\n",
            "2021-03-11 13:23:58,846 epoch 38 - iter 80/161 - loss 0.41868200 - samples/sec: 84.78 - lr: 0.003125\n",
            "2021-03-11 13:24:04,632 epoch 38 - iter 96/161 - loss 0.41073199 - samples/sec: 88.53 - lr: 0.003125\n",
            "2021-03-11 13:24:10,464 epoch 38 - iter 112/161 - loss 0.41580995 - samples/sec: 87.83 - lr: 0.003125\n",
            "2021-03-11 13:24:16,377 epoch 38 - iter 128/161 - loss 0.41452266 - samples/sec: 86.62 - lr: 0.003125\n",
            "2021-03-11 13:24:22,194 epoch 38 - iter 144/161 - loss 0.42190863 - samples/sec: 88.05 - lr: 0.003125\n",
            "2021-03-11 13:24:28,039 epoch 38 - iter 160/161 - loss 0.41987302 - samples/sec: 87.64 - lr: 0.003125\n",
            "2021-03-11 13:24:28,176 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:24:28,177 EPOCH 38 done: loss 0.4199 - lr 0.0031250\n",
            "2021-03-11 13:24:30,724 DEV : loss 0.6528280377388 - score 0.9145\n",
            "2021-03-11 13:24:30,762 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:24:30,763 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:24:36,614 epoch 39 - iter 16/161 - loss 0.37437847 - samples/sec: 87.57 - lr: 0.003125\n",
            "2021-03-11 13:24:42,432 epoch 39 - iter 32/161 - loss 0.42416080 - samples/sec: 88.04 - lr: 0.003125\n",
            "2021-03-11 13:24:48,289 epoch 39 - iter 48/161 - loss 0.42766699 - samples/sec: 87.46 - lr: 0.003125\n",
            "2021-03-11 13:24:53,994 epoch 39 - iter 64/161 - loss 0.42288539 - samples/sec: 89.78 - lr: 0.003125\n",
            "2021-03-11 13:25:00,007 epoch 39 - iter 80/161 - loss 0.41265455 - samples/sec: 85.18 - lr: 0.003125\n",
            "2021-03-11 13:25:05,807 epoch 39 - iter 96/161 - loss 0.40396402 - samples/sec: 88.32 - lr: 0.003125\n",
            "2021-03-11 13:25:11,787 epoch 39 - iter 112/161 - loss 0.40354045 - samples/sec: 85.65 - lr: 0.003125\n",
            "2021-03-11 13:25:17,608 epoch 39 - iter 128/161 - loss 0.40112444 - samples/sec: 88.00 - lr: 0.003125\n",
            "2021-03-11 13:25:23,674 epoch 39 - iter 144/161 - loss 0.41065746 - samples/sec: 84.43 - lr: 0.003125\n",
            "2021-03-11 13:25:29,434 epoch 39 - iter 160/161 - loss 0.40805023 - samples/sec: 88.92 - lr: 0.003125\n",
            "2021-03-11 13:25:29,558 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:25:29,560 EPOCH 39 done: loss 0.4075 - lr 0.0031250\n",
            "2021-03-11 13:25:32,100 DEV : loss 0.6529477834701538 - score 0.9158\n",
            "2021-03-11 13:25:32,139 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:25:32,140 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:25:38,049 epoch 40 - iter 16/161 - loss 0.37803325 - samples/sec: 86.70 - lr: 0.003125\n",
            "2021-03-11 13:25:43,909 epoch 40 - iter 32/161 - loss 0.37194064 - samples/sec: 87.41 - lr: 0.003125\n",
            "2021-03-11 13:25:49,629 epoch 40 - iter 48/161 - loss 0.38138578 - samples/sec: 89.54 - lr: 0.003125\n",
            "2021-03-11 13:25:55,474 epoch 40 - iter 64/161 - loss 0.38039071 - samples/sec: 87.65 - lr: 0.003125\n",
            "2021-03-11 13:26:01,359 epoch 40 - iter 80/161 - loss 0.38796535 - samples/sec: 87.03 - lr: 0.003125\n",
            "2021-03-11 13:26:07,316 epoch 40 - iter 96/161 - loss 0.38596693 - samples/sec: 85.99 - lr: 0.003125\n",
            "2021-03-11 13:26:13,170 epoch 40 - iter 112/161 - loss 0.38945679 - samples/sec: 87.49 - lr: 0.003125\n",
            "2021-03-11 13:26:19,049 epoch 40 - iter 128/161 - loss 0.40114628 - samples/sec: 87.13 - lr: 0.003125\n",
            "2021-03-11 13:26:24,950 epoch 40 - iter 144/161 - loss 0.39898200 - samples/sec: 86.80 - lr: 0.003125\n",
            "2021-03-11 13:26:30,813 epoch 40 - iter 160/161 - loss 0.40808623 - samples/sec: 87.36 - lr: 0.003125\n",
            "2021-03-11 13:26:30,926 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:26:30,927 EPOCH 40 done: loss 0.4063 - lr 0.0031250\n",
            "2021-03-11 13:26:33,454 DEV : loss 0.6533915400505066 - score 0.9158\n",
            "Epoch    40: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2021-03-11 13:26:33,493 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:26:33,494 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:26:39,281 epoch 41 - iter 16/161 - loss 0.35148946 - samples/sec: 88.53 - lr: 0.001563\n",
            "2021-03-11 13:26:45,182 epoch 41 - iter 32/161 - loss 0.35989678 - samples/sec: 86.80 - lr: 0.001563\n",
            "2021-03-11 13:26:51,058 epoch 41 - iter 48/161 - loss 0.38159655 - samples/sec: 87.18 - lr: 0.001563\n",
            "2021-03-11 13:26:56,880 epoch 41 - iter 64/161 - loss 0.38448081 - samples/sec: 87.97 - lr: 0.001563\n",
            "2021-03-11 13:27:02,717 epoch 41 - iter 80/161 - loss 0.39093969 - samples/sec: 87.76 - lr: 0.001563\n",
            "2021-03-11 13:27:08,568 epoch 41 - iter 96/161 - loss 0.38967868 - samples/sec: 87.54 - lr: 0.001563\n",
            "2021-03-11 13:27:14,379 epoch 41 - iter 112/161 - loss 0.38491341 - samples/sec: 88.14 - lr: 0.001563\n",
            "2021-03-11 13:27:20,271 epoch 41 - iter 128/161 - loss 0.38404010 - samples/sec: 86.93 - lr: 0.001563\n",
            "2021-03-11 13:27:26,144 epoch 41 - iter 144/161 - loss 0.39846839 - samples/sec: 87.21 - lr: 0.001563\n",
            "2021-03-11 13:27:32,129 epoch 41 - iter 160/161 - loss 0.39825906 - samples/sec: 85.58 - lr: 0.001563\n",
            "2021-03-11 13:27:32,255 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:27:32,256 EPOCH 41 done: loss 0.3973 - lr 0.0015625\n",
            "2021-03-11 13:27:34,784 DEV : loss 0.6528453230857849 - score 0.9153\n",
            "2021-03-11 13:27:34,824 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:27:34,825 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:27:40,682 epoch 42 - iter 16/161 - loss 0.42039922 - samples/sec: 87.48 - lr: 0.001563\n",
            "2021-03-11 13:27:46,600 epoch 42 - iter 32/161 - loss 0.37762450 - samples/sec: 86.55 - lr: 0.001563\n",
            "2021-03-11 13:27:52,528 epoch 42 - iter 48/161 - loss 0.37959661 - samples/sec: 86.42 - lr: 0.001563\n",
            "2021-03-11 13:27:58,385 epoch 42 - iter 64/161 - loss 0.37492020 - samples/sec: 87.45 - lr: 0.001563\n",
            "2021-03-11 13:28:04,270 epoch 42 - iter 80/161 - loss 0.38571255 - samples/sec: 87.04 - lr: 0.001563\n",
            "2021-03-11 13:28:10,148 epoch 42 - iter 96/161 - loss 0.37475633 - samples/sec: 87.13 - lr: 0.001563\n",
            "2021-03-11 13:28:16,067 epoch 42 - iter 112/161 - loss 0.38127644 - samples/sec: 86.54 - lr: 0.001563\n",
            "2021-03-11 13:28:22,036 epoch 42 - iter 128/161 - loss 0.38879648 - samples/sec: 85.83 - lr: 0.001563\n",
            "2021-03-11 13:28:27,885 epoch 42 - iter 144/161 - loss 0.40171361 - samples/sec: 87.56 - lr: 0.001563\n",
            "2021-03-11 13:28:33,718 epoch 42 - iter 160/161 - loss 0.40472757 - samples/sec: 87.81 - lr: 0.001563\n",
            "2021-03-11 13:28:33,835 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:28:33,836 EPOCH 42 done: loss 0.4061 - lr 0.0015625\n",
            "2021-03-11 13:28:36,357 DEV : loss 0.6524314284324646 - score 0.9146\n",
            "2021-03-11 13:28:36,397 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:28:36,398 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:28:42,197 epoch 43 - iter 16/161 - loss 0.47145544 - samples/sec: 88.35 - lr: 0.001563\n",
            "2021-03-11 13:28:48,042 epoch 43 - iter 32/161 - loss 0.40699505 - samples/sec: 87.63 - lr: 0.001563\n",
            "2021-03-11 13:28:53,874 epoch 43 - iter 48/161 - loss 0.41109153 - samples/sec: 87.83 - lr: 0.001563\n",
            "2021-03-11 13:28:59,524 epoch 43 - iter 64/161 - loss 0.40691696 - samples/sec: 90.65 - lr: 0.001563\n",
            "2021-03-11 13:29:05,388 epoch 43 - iter 80/161 - loss 0.41205985 - samples/sec: 87.36 - lr: 0.001563\n",
            "2021-03-11 13:29:11,374 epoch 43 - iter 96/161 - loss 0.41311693 - samples/sec: 85.56 - lr: 0.001563\n",
            "2021-03-11 13:29:17,204 epoch 43 - iter 112/161 - loss 0.41352004 - samples/sec: 87.86 - lr: 0.001563\n",
            "2021-03-11 13:29:23,189 epoch 43 - iter 128/161 - loss 0.41998264 - samples/sec: 85.58 - lr: 0.001563\n",
            "2021-03-11 13:29:29,099 epoch 43 - iter 144/161 - loss 0.42185045 - samples/sec: 86.66 - lr: 0.001563\n",
            "2021-03-11 13:29:34,977 epoch 43 - iter 160/161 - loss 0.41552739 - samples/sec: 87.14 - lr: 0.001563\n",
            "2021-03-11 13:29:35,111 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:29:35,112 EPOCH 43 done: loss 0.4151 - lr 0.0015625\n",
            "2021-03-11 13:29:37,646 DEV : loss 0.6522425413131714 - score 0.9146\n",
            "2021-03-11 13:29:37,686 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:29:37,687 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:29:43,364 epoch 44 - iter 16/161 - loss 0.40235595 - samples/sec: 90.25 - lr: 0.001563\n",
            "2021-03-11 13:29:49,316 epoch 44 - iter 32/161 - loss 0.38581532 - samples/sec: 86.05 - lr: 0.001563\n",
            "2021-03-11 13:29:55,252 epoch 44 - iter 48/161 - loss 0.39348252 - samples/sec: 86.29 - lr: 0.001563\n",
            "2021-03-11 13:30:01,001 epoch 44 - iter 64/161 - loss 0.39539317 - samples/sec: 89.10 - lr: 0.001563\n",
            "2021-03-11 13:30:06,980 epoch 44 - iter 80/161 - loss 0.39547946 - samples/sec: 85.67 - lr: 0.001563\n",
            "2021-03-11 13:30:12,917 epoch 44 - iter 96/161 - loss 0.39585701 - samples/sec: 86.27 - lr: 0.001563\n",
            "2021-03-11 13:30:18,739 epoch 44 - iter 112/161 - loss 0.40765158 - samples/sec: 87.97 - lr: 0.001563\n",
            "2021-03-11 13:30:24,634 epoch 44 - iter 128/161 - loss 0.39693649 - samples/sec: 86.90 - lr: 0.001563\n",
            "2021-03-11 13:30:30,582 epoch 44 - iter 144/161 - loss 0.39831669 - samples/sec: 86.11 - lr: 0.001563\n",
            "2021-03-11 13:30:36,449 epoch 44 - iter 160/161 - loss 0.40667027 - samples/sec: 87.30 - lr: 0.001563\n",
            "2021-03-11 13:30:36,589 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:30:36,589 EPOCH 44 done: loss 0.4052 - lr 0.0015625\n",
            "2021-03-11 13:30:39,111 DEV : loss 0.6535018682479858 - score 0.9141\n",
            "Epoch    44: reducing learning rate of group 0 to 7.8125e-04.\n",
            "2021-03-11 13:30:39,154 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:30:39,155 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:30:44,961 epoch 45 - iter 16/161 - loss 0.38329846 - samples/sec: 88.24 - lr: 0.000781\n",
            "2021-03-11 13:30:50,813 epoch 45 - iter 32/161 - loss 0.38536132 - samples/sec: 87.53 - lr: 0.000781\n",
            "2021-03-11 13:30:56,574 epoch 45 - iter 48/161 - loss 0.41822529 - samples/sec: 88.92 - lr: 0.000781\n",
            "2021-03-11 13:31:02,343 epoch 45 - iter 64/161 - loss 0.40004252 - samples/sec: 88.79 - lr: 0.000781\n",
            "2021-03-11 13:31:08,202 epoch 45 - iter 80/161 - loss 0.39189079 - samples/sec: 87.42 - lr: 0.000781\n",
            "2021-03-11 13:31:14,094 epoch 45 - iter 96/161 - loss 0.40187164 - samples/sec: 86.92 - lr: 0.000781\n",
            "2021-03-11 13:31:19,977 epoch 45 - iter 112/161 - loss 0.39983541 - samples/sec: 87.07 - lr: 0.000781\n",
            "2021-03-11 13:31:25,640 epoch 45 - iter 128/161 - loss 0.40286081 - samples/sec: 90.45 - lr: 0.000781\n",
            "2021-03-11 13:31:31,589 epoch 45 - iter 144/161 - loss 0.40127264 - samples/sec: 86.09 - lr: 0.000781\n",
            "2021-03-11 13:31:37,650 epoch 45 - iter 160/161 - loss 0.39928033 - samples/sec: 84.51 - lr: 0.000781\n",
            "2021-03-11 13:31:37,798 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:31:37,799 EPOCH 45 done: loss 0.4002 - lr 0.0007813\n",
            "2021-03-11 13:31:40,397 DEV : loss 0.6537163853645325 - score 0.9146\n",
            "2021-03-11 13:31:40,435 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:31:40,436 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:31:46,466 epoch 46 - iter 16/161 - loss 0.36168868 - samples/sec: 84.98 - lr: 0.000781\n",
            "2021-03-11 13:31:52,277 epoch 46 - iter 32/161 - loss 0.36197341 - samples/sec: 88.15 - lr: 0.000781\n",
            "2021-03-11 13:31:58,226 epoch 46 - iter 48/161 - loss 0.35331133 - samples/sec: 86.10 - lr: 0.000781\n",
            "2021-03-11 13:32:04,056 epoch 46 - iter 64/161 - loss 0.37014654 - samples/sec: 87.85 - lr: 0.000781\n",
            "2021-03-11 13:32:09,908 epoch 46 - iter 80/161 - loss 0.38142366 - samples/sec: 87.54 - lr: 0.000781\n",
            "2021-03-11 13:32:15,744 epoch 46 - iter 96/161 - loss 0.37950034 - samples/sec: 87.76 - lr: 0.000781\n",
            "2021-03-11 13:32:21,622 epoch 46 - iter 112/161 - loss 0.38650232 - samples/sec: 87.14 - lr: 0.000781\n",
            "2021-03-11 13:32:27,556 epoch 46 - iter 128/161 - loss 0.39612474 - samples/sec: 86.32 - lr: 0.000781\n",
            "2021-03-11 13:32:33,363 epoch 46 - iter 144/161 - loss 0.40184494 - samples/sec: 88.21 - lr: 0.000781\n",
            "2021-03-11 13:32:39,095 epoch 46 - iter 160/161 - loss 0.40634106 - samples/sec: 89.35 - lr: 0.000781\n",
            "2021-03-11 13:32:39,228 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:32:39,229 EPOCH 46 done: loss 0.4060 - lr 0.0007813\n",
            "2021-03-11 13:32:41,785 DEV : loss 0.6534948348999023 - score 0.9146\n",
            "2021-03-11 13:32:41,828 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:32:41,829 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:32:47,751 epoch 47 - iter 16/161 - loss 0.46247139 - samples/sec: 86.52 - lr: 0.000781\n",
            "2021-03-11 13:32:53,708 epoch 47 - iter 32/161 - loss 0.43984736 - samples/sec: 86.00 - lr: 0.000781\n",
            "2021-03-11 13:32:59,512 epoch 47 - iter 48/161 - loss 0.40940385 - samples/sec: 88.24 - lr: 0.000781\n",
            "2021-03-11 13:33:05,244 epoch 47 - iter 64/161 - loss 0.39887541 - samples/sec: 89.36 - lr: 0.000781\n",
            "2021-03-11 13:33:11,381 epoch 47 - iter 80/161 - loss 0.40129133 - samples/sec: 83.47 - lr: 0.000781\n",
            "2021-03-11 13:33:17,434 epoch 47 - iter 96/161 - loss 0.40851303 - samples/sec: 84.61 - lr: 0.000781\n",
            "2021-03-11 13:33:23,260 epoch 47 - iter 112/161 - loss 0.41132665 - samples/sec: 87.93 - lr: 0.000781\n",
            "2021-03-11 13:33:29,147 epoch 47 - iter 128/161 - loss 0.40386374 - samples/sec: 87.00 - lr: 0.000781\n",
            "2021-03-11 13:33:34,947 epoch 47 - iter 144/161 - loss 0.40873953 - samples/sec: 88.32 - lr: 0.000781\n",
            "2021-03-11 13:33:40,863 epoch 47 - iter 160/161 - loss 0.41092291 - samples/sec: 86.57 - lr: 0.000781\n",
            "2021-03-11 13:33:41,000 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:33:41,001 EPOCH 47 done: loss 0.4120 - lr 0.0007813\n",
            "2021-03-11 13:33:43,556 DEV : loss 0.6531400084495544 - score 0.9145\n",
            "2021-03-11 13:33:43,596 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:33:43,597 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:33:49,377 epoch 48 - iter 16/161 - loss 0.41209524 - samples/sec: 88.65 - lr: 0.000781\n",
            "2021-03-11 13:33:55,251 epoch 48 - iter 32/161 - loss 0.40838042 - samples/sec: 87.20 - lr: 0.000781\n",
            "2021-03-11 13:34:01,043 epoch 48 - iter 48/161 - loss 0.40019302 - samples/sec: 88.43 - lr: 0.000781\n",
            "2021-03-11 13:34:06,820 epoch 48 - iter 64/161 - loss 0.39581986 - samples/sec: 88.66 - lr: 0.000781\n",
            "2021-03-11 13:34:12,788 epoch 48 - iter 80/161 - loss 0.40689401 - samples/sec: 85.83 - lr: 0.000781\n",
            "2021-03-11 13:34:18,849 epoch 48 - iter 96/161 - loss 0.40889869 - samples/sec: 84.50 - lr: 0.000781\n",
            "2021-03-11 13:34:24,715 epoch 48 - iter 112/161 - loss 0.39994918 - samples/sec: 87.32 - lr: 0.000781\n",
            "2021-03-11 13:34:30,621 epoch 48 - iter 128/161 - loss 0.39831409 - samples/sec: 86.73 - lr: 0.000781\n",
            "2021-03-11 13:34:36,433 epoch 48 - iter 144/161 - loss 0.40092041 - samples/sec: 88.13 - lr: 0.000781\n",
            "2021-03-11 13:34:42,379 epoch 48 - iter 160/161 - loss 0.40174248 - samples/sec: 86.15 - lr: 0.000781\n",
            "2021-03-11 13:34:42,518 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:34:42,519 EPOCH 48 done: loss 0.4019 - lr 0.0007813\n",
            "2021-03-11 13:34:45,021 DEV : loss 0.6548898220062256 - score 0.914\n",
            "Epoch    48: reducing learning rate of group 0 to 3.9063e-04.\n",
            "2021-03-11 13:34:45,061 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:34:45,063 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:34:51,106 epoch 49 - iter 16/161 - loss 0.47463640 - samples/sec: 84.80 - lr: 0.000391\n",
            "2021-03-11 13:34:56,964 epoch 49 - iter 32/161 - loss 0.44962195 - samples/sec: 87.44 - lr: 0.000391\n",
            "2021-03-11 13:35:02,864 epoch 49 - iter 48/161 - loss 0.44925735 - samples/sec: 86.81 - lr: 0.000391\n",
            "2021-03-11 13:35:08,629 epoch 49 - iter 64/161 - loss 0.44775243 - samples/sec: 88.84 - lr: 0.000391\n",
            "2021-03-11 13:35:14,470 epoch 49 - iter 80/161 - loss 0.42575909 - samples/sec: 87.70 - lr: 0.000391\n",
            "2021-03-11 13:35:20,302 epoch 49 - iter 96/161 - loss 0.42594310 - samples/sec: 87.83 - lr: 0.000391\n",
            "2021-03-11 13:35:26,081 epoch 49 - iter 112/161 - loss 0.42855482 - samples/sec: 88.63 - lr: 0.000391\n",
            "2021-03-11 13:35:32,061 epoch 49 - iter 128/161 - loss 0.41920997 - samples/sec: 85.65 - lr: 0.000391\n",
            "2021-03-11 13:35:37,827 epoch 49 - iter 144/161 - loss 0.42124531 - samples/sec: 88.83 - lr: 0.000391\n",
            "2021-03-11 13:35:43,719 epoch 49 - iter 160/161 - loss 0.41941921 - samples/sec: 86.93 - lr: 0.000391\n",
            "2021-03-11 13:35:43,837 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:35:43,838 EPOCH 49 done: loss 0.4200 - lr 0.0003906\n",
            "2021-03-11 13:35:46,380 DEV : loss 0.654450535774231 - score 0.9145\n",
            "2021-03-11 13:35:46,419 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:35:46,420 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:35:52,288 epoch 50 - iter 16/161 - loss 0.40539638 - samples/sec: 87.33 - lr: 0.000391\n",
            "2021-03-11 13:35:58,085 epoch 50 - iter 32/161 - loss 0.40498911 - samples/sec: 88.36 - lr: 0.000391\n",
            "2021-03-11 13:36:04,025 epoch 50 - iter 48/161 - loss 0.40727843 - samples/sec: 86.23 - lr: 0.000391\n",
            "2021-03-11 13:36:09,909 epoch 50 - iter 64/161 - loss 0.39364117 - samples/sec: 87.05 - lr: 0.000391\n",
            "2021-03-11 13:36:15,702 epoch 50 - iter 80/161 - loss 0.39700039 - samples/sec: 88.42 - lr: 0.000391\n",
            "2021-03-11 13:36:21,725 epoch 50 - iter 96/161 - loss 0.40179193 - samples/sec: 85.04 - lr: 0.000391\n",
            "2021-03-11 13:36:27,609 epoch 50 - iter 112/161 - loss 0.40129942 - samples/sec: 87.06 - lr: 0.000391\n",
            "2021-03-11 13:36:33,364 epoch 50 - iter 128/161 - loss 0.41359998 - samples/sec: 89.00 - lr: 0.000391\n",
            "2021-03-11 13:36:39,468 epoch 50 - iter 144/161 - loss 0.41648031 - samples/sec: 83.92 - lr: 0.000391\n",
            "2021-03-11 13:36:45,216 epoch 50 - iter 160/161 - loss 0.41343813 - samples/sec: 89.11 - lr: 0.000391\n",
            "2021-03-11 13:36:45,353 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:36:45,354 EPOCH 50 done: loss 0.4160 - lr 0.0003906\n",
            "2021-03-11 13:36:47,895 DEV : loss 0.6547327041625977 - score 0.914\n",
            "2021-03-11 13:36:47,935 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:36:47,937 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:36:53,555 epoch 51 - iter 16/161 - loss 0.40834197 - samples/sec: 91.21 - lr: 0.000391\n",
            "2021-03-11 13:36:59,632 epoch 51 - iter 32/161 - loss 0.43083437 - samples/sec: 84.29 - lr: 0.000391\n",
            "2021-03-11 13:37:05,540 epoch 51 - iter 48/161 - loss 0.42422101 - samples/sec: 86.69 - lr: 0.000391\n",
            "2021-03-11 13:37:11,505 epoch 51 - iter 64/161 - loss 0.42917227 - samples/sec: 85.88 - lr: 0.000391\n",
            "2021-03-11 13:37:17,327 epoch 51 - iter 80/161 - loss 0.41079447 - samples/sec: 87.97 - lr: 0.000391\n",
            "2021-03-11 13:37:23,194 epoch 51 - iter 96/161 - loss 0.40537817 - samples/sec: 87.31 - lr: 0.000391\n",
            "2021-03-11 13:37:29,018 epoch 51 - iter 112/161 - loss 0.40480006 - samples/sec: 87.94 - lr: 0.000391\n",
            "2021-03-11 13:37:34,900 epoch 51 - iter 128/161 - loss 0.40578809 - samples/sec: 87.09 - lr: 0.000391\n",
            "2021-03-11 13:37:40,699 epoch 51 - iter 144/161 - loss 0.40243490 - samples/sec: 88.34 - lr: 0.000391\n",
            "2021-03-11 13:37:46,611 epoch 51 - iter 160/161 - loss 0.39647673 - samples/sec: 86.64 - lr: 0.000391\n",
            "2021-03-11 13:37:46,730 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:37:46,731 EPOCH 51 done: loss 0.3962 - lr 0.0003906\n",
            "2021-03-11 13:37:49,269 DEV : loss 0.654417872428894 - score 0.914\n",
            "2021-03-11 13:37:49,307 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:37:49,308 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:37:55,265 epoch 52 - iter 16/161 - loss 0.46724086 - samples/sec: 86.01 - lr: 0.000391\n",
            "2021-03-11 13:38:01,193 epoch 52 - iter 32/161 - loss 0.45249601 - samples/sec: 86.40 - lr: 0.000391\n",
            "2021-03-11 13:38:06,966 epoch 52 - iter 48/161 - loss 0.42528699 - samples/sec: 88.73 - lr: 0.000391\n",
            "2021-03-11 13:38:12,779 epoch 52 - iter 64/161 - loss 0.41941795 - samples/sec: 88.12 - lr: 0.000391\n",
            "2021-03-11 13:38:19,915 epoch 52 - iter 80/161 - loss 0.42528243 - samples/sec: 71.78 - lr: 0.000391\n",
            "2021-03-11 13:38:25,856 epoch 52 - iter 96/161 - loss 0.42071048 - samples/sec: 86.21 - lr: 0.000391\n",
            "2021-03-11 13:38:31,625 epoch 52 - iter 112/161 - loss 0.40916830 - samples/sec: 88.79 - lr: 0.000391\n",
            "2021-03-11 13:38:37,412 epoch 52 - iter 128/161 - loss 0.40992857 - samples/sec: 88.51 - lr: 0.000391\n",
            "2021-03-11 13:38:43,214 epoch 52 - iter 144/161 - loss 0.41236875 - samples/sec: 88.28 - lr: 0.000391\n",
            "2021-03-11 13:38:49,150 epoch 52 - iter 160/161 - loss 0.41558474 - samples/sec: 86.28 - lr: 0.000391\n",
            "2021-03-11 13:38:49,253 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:38:49,254 EPOCH 52 done: loss 0.4157 - lr 0.0003906\n",
            "2021-03-11 13:38:51,790 DEV : loss 0.6536942720413208 - score 0.914\n",
            "Epoch    52: reducing learning rate of group 0 to 1.9531e-04.\n",
            "2021-03-11 13:38:51,829 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:38:51,830 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:38:57,639 epoch 53 - iter 16/161 - loss 0.45247906 - samples/sec: 88.21 - lr: 0.000195\n",
            "2021-03-11 13:39:03,543 epoch 53 - iter 32/161 - loss 0.42328250 - samples/sec: 86.75 - lr: 0.000195\n",
            "2021-03-11 13:39:09,366 epoch 53 - iter 48/161 - loss 0.40317170 - samples/sec: 87.97 - lr: 0.000195\n",
            "2021-03-11 13:39:15,251 epoch 53 - iter 64/161 - loss 0.39326948 - samples/sec: 87.03 - lr: 0.000195\n",
            "2021-03-11 13:39:21,188 epoch 53 - iter 80/161 - loss 0.38609801 - samples/sec: 86.27 - lr: 0.000195\n",
            "2021-03-11 13:39:26,856 epoch 53 - iter 96/161 - loss 0.39387582 - samples/sec: 90.37 - lr: 0.000195\n",
            "2021-03-11 13:39:32,843 epoch 53 - iter 112/161 - loss 0.39593100 - samples/sec: 85.55 - lr: 0.000195\n",
            "2021-03-11 13:39:38,737 epoch 53 - iter 128/161 - loss 0.39312670 - samples/sec: 86.90 - lr: 0.000195\n",
            "2021-03-11 13:39:44,601 epoch 53 - iter 144/161 - loss 0.39129674 - samples/sec: 87.35 - lr: 0.000195\n",
            "2021-03-11 13:39:50,465 epoch 53 - iter 160/161 - loss 0.39651014 - samples/sec: 87.35 - lr: 0.000195\n",
            "2021-03-11 13:39:50,576 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:39:50,576 EPOCH 53 done: loss 0.3963 - lr 0.0001953\n",
            "2021-03-11 13:39:53,131 DEV : loss 0.6540732979774475 - score 0.914\n",
            "2021-03-11 13:39:53,173 BAD EPOCHS (no improvement): 1\n",
            "2021-03-11 13:39:53,174 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:39:59,186 epoch 54 - iter 16/161 - loss 0.45083394 - samples/sec: 85.22 - lr: 0.000195\n",
            "2021-03-11 13:40:04,862 epoch 54 - iter 32/161 - loss 0.45201520 - samples/sec: 90.24 - lr: 0.000195\n",
            "2021-03-11 13:40:10,755 epoch 54 - iter 48/161 - loss 0.43431861 - samples/sec: 86.92 - lr: 0.000195\n",
            "2021-03-11 13:40:16,581 epoch 54 - iter 64/161 - loss 0.43689414 - samples/sec: 87.92 - lr: 0.000195\n",
            "2021-03-11 13:40:22,614 epoch 54 - iter 80/161 - loss 0.42752793 - samples/sec: 84.91 - lr: 0.000195\n",
            "2021-03-11 13:40:28,581 epoch 54 - iter 96/161 - loss 0.41641757 - samples/sec: 85.84 - lr: 0.000195\n",
            "2021-03-11 13:40:34,432 epoch 54 - iter 112/161 - loss 0.40754556 - samples/sec: 87.54 - lr: 0.000195\n",
            "2021-03-11 13:40:40,396 epoch 54 - iter 128/161 - loss 0.41406207 - samples/sec: 85.89 - lr: 0.000195\n",
            "2021-03-11 13:40:46,301 epoch 54 - iter 144/161 - loss 0.41291092 - samples/sec: 86.74 - lr: 0.000195\n",
            "2021-03-11 13:40:52,067 epoch 54 - iter 160/161 - loss 0.41300141 - samples/sec: 88.83 - lr: 0.000195\n",
            "2021-03-11 13:40:52,175 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:40:52,175 EPOCH 54 done: loss 0.4117 - lr 0.0001953\n",
            "2021-03-11 13:40:54,795 DEV : loss 0.6541817784309387 - score 0.914\n",
            "2021-03-11 13:40:54,834 BAD EPOCHS (no improvement): 2\n",
            "2021-03-11 13:40:54,836 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:41:00,896 epoch 55 - iter 16/161 - loss 0.32746719 - samples/sec: 84.54 - lr: 0.000195\n",
            "2021-03-11 13:41:06,761 epoch 55 - iter 32/161 - loss 0.39575977 - samples/sec: 87.34 - lr: 0.000195\n",
            "2021-03-11 13:41:12,653 epoch 55 - iter 48/161 - loss 0.41640278 - samples/sec: 86.94 - lr: 0.000195\n",
            "2021-03-11 13:41:18,571 epoch 55 - iter 64/161 - loss 0.41489743 - samples/sec: 86.55 - lr: 0.000195\n",
            "2021-03-11 13:41:24,437 epoch 55 - iter 80/161 - loss 0.42379011 - samples/sec: 87.32 - lr: 0.000195\n",
            "2021-03-11 13:41:30,102 epoch 55 - iter 96/161 - loss 0.41503735 - samples/sec: 90.41 - lr: 0.000195\n",
            "2021-03-11 13:41:35,978 epoch 55 - iter 112/161 - loss 0.40746125 - samples/sec: 87.18 - lr: 0.000195\n",
            "2021-03-11 13:41:41,763 epoch 55 - iter 128/161 - loss 0.41190464 - samples/sec: 88.54 - lr: 0.000195\n",
            "2021-03-11 13:41:47,599 epoch 55 - iter 144/161 - loss 0.40989940 - samples/sec: 87.77 - lr: 0.000195\n",
            "2021-03-11 13:41:53,405 epoch 55 - iter 160/161 - loss 0.40693126 - samples/sec: 88.22 - lr: 0.000195\n",
            "2021-03-11 13:41:53,540 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:41:53,541 EPOCH 55 done: loss 0.4059 - lr 0.0001953\n",
            "2021-03-11 13:41:56,061 DEV : loss 0.6544028520584106 - score 0.914\n",
            "2021-03-11 13:41:56,101 BAD EPOCHS (no improvement): 3\n",
            "2021-03-11 13:41:56,102 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:42:02,040 epoch 56 - iter 16/161 - loss 0.46342899 - samples/sec: 86.29 - lr: 0.000195\n",
            "2021-03-11 13:42:07,901 epoch 56 - iter 32/161 - loss 0.45265277 - samples/sec: 87.39 - lr: 0.000195\n",
            "2021-03-11 13:42:13,724 epoch 56 - iter 48/161 - loss 0.41397164 - samples/sec: 87.97 - lr: 0.000195\n",
            "2021-03-11 13:42:19,492 epoch 56 - iter 64/161 - loss 0.42609874 - samples/sec: 88.80 - lr: 0.000195\n",
            "2021-03-11 13:42:25,391 epoch 56 - iter 80/161 - loss 0.42353043 - samples/sec: 86.84 - lr: 0.000195\n",
            "2021-03-11 13:42:31,237 epoch 56 - iter 96/161 - loss 0.42956919 - samples/sec: 87.62 - lr: 0.000195\n",
            "2021-03-11 13:42:37,216 epoch 56 - iter 112/161 - loss 0.41634650 - samples/sec: 85.67 - lr: 0.000195\n",
            "2021-03-11 13:42:43,061 epoch 56 - iter 128/161 - loss 0.41353562 - samples/sec: 87.63 - lr: 0.000195\n",
            "2021-03-11 13:42:48,845 epoch 56 - iter 144/161 - loss 0.41159632 - samples/sec: 88.54 - lr: 0.000195\n",
            "2021-03-11 13:42:54,636 epoch 56 - iter 160/161 - loss 0.41552112 - samples/sec: 88.46 - lr: 0.000195\n",
            "2021-03-11 13:42:54,763 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:42:54,764 EPOCH 56 done: loss 0.4154 - lr 0.0001953\n",
            "2021-03-11 13:42:57,277 DEV : loss 0.6546640992164612 - score 0.914\n",
            "Epoch    56: reducing learning rate of group 0 to 9.7656e-05.\n",
            "2021-03-11 13:42:57,318 BAD EPOCHS (no improvement): 4\n",
            "2021-03-11 13:42:57,319 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:42:57,320 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:42:57,321 learning rate too small - quitting training!\n",
            "2021-03-11 13:42:57,322 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:44:25,751 ----------------------------------------------------------------------------------------------------\n",
            "2021-03-11 13:44:29,928 Testing using best model ...\n",
            "2021-03-11 13:44:31,664 loading file resources/taggers/example-ner/best-model.pt\n",
            "2021-03-11 13:46:55,891 0.8125\t0.7335\t0.7710\n",
            "2021-03-11 13:46:55,896 \n",
            "Results:\n",
            "- F1-score (micro) 0.7710\n",
            "- F1-score (macro) 0.7710\n",
            "\n",
            "By class:\n",
            "location   tp: 234 - fp: 54 - fn: 85 - precision: 0.8125 - recall: 0.7335 - f1-score: 0.7710\n",
            "2021-03-11 13:46:55,896 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [2.9712154865264893,\n",
              "  0.9905304312705994,\n",
              "  0.8376005291938782,\n",
              "  1.0136915445327759,\n",
              "  0.7377455830574036,\n",
              "  0.7242583632469177,\n",
              "  0.6750032305717468,\n",
              "  0.6758297085762024,\n",
              "  0.6810588836669922,\n",
              "  0.6823708415031433,\n",
              "  0.8654462099075317,\n",
              "  0.6553251147270203,\n",
              "  0.7124163508415222,\n",
              "  0.6431124806404114,\n",
              "  0.6884129047393799,\n",
              "  0.7235537767410278,\n",
              "  0.6823051571846008,\n",
              "  0.6383929252624512,\n",
              "  0.6734601855278015,\n",
              "  0.6518872976303101,\n",
              "  0.6605895161628723,\n",
              "  0.6677911877632141,\n",
              "  0.6627979874610901,\n",
              "  0.6384405493736267,\n",
              "  0.6412671804428101,\n",
              "  0.6555718779563904,\n",
              "  0.6319429278373718,\n",
              "  0.6334441304206848,\n",
              "  0.6346413493156433,\n",
              "  0.6459820866584778,\n",
              "  0.6542726159095764,\n",
              "  0.6493107676506042,\n",
              "  0.6523300409317017,\n",
              "  0.6480966806411743,\n",
              "  0.652953565120697,\n",
              "  0.6584500670433044,\n",
              "  0.6537882685661316,\n",
              "  0.6528280377388,\n",
              "  0.6529477834701538,\n",
              "  0.6533915400505066,\n",
              "  0.6528453230857849,\n",
              "  0.6524314284324646,\n",
              "  0.6522425413131714,\n",
              "  0.6535018682479858,\n",
              "  0.6537163853645325,\n",
              "  0.6534948348999023,\n",
              "  0.6531400084495544,\n",
              "  0.6548898220062256,\n",
              "  0.654450535774231,\n",
              "  0.6547327041625977,\n",
              "  0.654417872428894,\n",
              "  0.6536942720413208,\n",
              "  0.6540732979774475,\n",
              "  0.6541817784309387,\n",
              "  0.6544028520584106,\n",
              "  0.6546640992164612],\n",
              " 'dev_score_history': [0.6633941093969143,\n",
              "  0.8622327790973872,\n",
              "  0.8967057509771078,\n",
              "  0.8835257082896117,\n",
              "  0.9081803005008346,\n",
              "  0.9092884302009776,\n",
              "  0.9064425770308122,\n",
              "  0.9063376332024679,\n",
              "  0.914572864321608,\n",
              "  0.910902047592695,\n",
              "  0.8866200967221923,\n",
              "  0.9196919691969198,\n",
              "  0.9120699071545604,\n",
              "  0.914349276974416,\n",
              "  0.9112097669256382,\n",
              "  0.9117647058823529,\n",
              "  0.9099836333878888,\n",
              "  0.9164360819037078,\n",
              "  0.9161147902869757,\n",
              "  0.9238465814341301,\n",
              "  0.9159292035398231,\n",
              "  0.9157427937915743,\n",
              "  0.9146341463414634,\n",
              "  0.9161073825503355,\n",
              "  0.9194891726818434,\n",
              "  0.9178157749586322,\n",
              "  0.9231632080762759,\n",
              "  0.9164360819037078,\n",
              "  0.921612541993281,\n",
              "  0.9136212624584718,\n",
              "  0.9146341463414634,\n",
              "  0.9151414309484193,\n",
              "  0.9156492785793562,\n",
              "  0.9172681843420323,\n",
              "  0.9152354570637118,\n",
              "  0.9158361018826136,\n",
              "  0.9129229062673322,\n",
              "  0.9145394006659268,\n",
              "  0.9158361018826136,\n",
              "  0.9158361018826136,\n",
              "  0.9153292750415053,\n",
              "  0.9146341463414634,\n",
              "  0.9146341463414634,\n",
              "  0.9141274238227147,\n",
              "  0.9146341463414634,\n",
              "  0.9146341463414634,\n",
              "  0.9145394006659268,\n",
              "  0.9140321686078757,\n",
              "  0.9145394006659268,\n",
              "  0.9140321686078757,\n",
              "  0.9140321686078757,\n",
              "  0.9140321686078757,\n",
              "  0.9140321686078757,\n",
              "  0.9140321686078757,\n",
              "  0.9140321686078757,\n",
              "  0.9140321686078757],\n",
              " 'test_score': 0.7710049423393739,\n",
              " 'train_loss_history': [3.5925945947814433,\n",
              "  1.505231339738976,\n",
              "  1.207167632276227,\n",
              "  1.0529739945571615,\n",
              "  0.9619009407780925,\n",
              "  0.9254172346236543,\n",
              "  0.8614937896313875,\n",
              "  0.8434936048821633,\n",
              "  0.7998570916445359,\n",
              "  0.7507218498250713,\n",
              "  0.761795675532418,\n",
              "  0.7328932512991176,\n",
              "  0.6946718555799922,\n",
              "  0.6953428598664562,\n",
              "  0.6833693955255591,\n",
              "  0.6647906123851397,\n",
              "  0.5945158449018965,\n",
              "  0.5817906203107064,\n",
              "  0.5589778299287239,\n",
              "  0.5478665436276738,\n",
              "  0.5399927002672823,\n",
              "  0.5281223597363656,\n",
              "  0.5274923249431278,\n",
              "  0.5216747057734069,\n",
              "  0.4947569400812528,\n",
              "  0.46091826269345254,\n",
              "  0.4635873322161088,\n",
              "  0.45856177121956154,\n",
              "  0.4513656464607819,\n",
              "  0.42833093586175336,\n",
              "  0.4445289032799857,\n",
              "  0.4291827921171366,\n",
              "  0.43245607244302026,\n",
              "  0.43207345245787815,\n",
              "  0.4219698349696509,\n",
              "  0.4187880830364938,\n",
              "  0.40850505066214143,\n",
              "  0.4198796536241259,\n",
              "  0.40749176966477624,\n",
              "  0.4062718664822371,\n",
              "  0.39725970074256756,\n",
              "  0.406088242804782,\n",
              "  0.4150584391925646,\n",
              "  0.40524618503469856,\n",
              "  0.40020112637777505,\n",
              "  0.4059664940241701,\n",
              "  0.4119677562150896,\n",
              "  0.4018967259004249,\n",
              "  0.42003047540321115,\n",
              "  0.4160077630177788,\n",
              "  0.39622081677365745,\n",
              "  0.4157159391397275,\n",
              "  0.3963287956781269,\n",
              "  0.4116533789205255,\n",
              "  0.4059483932782404,\n",
              "  0.41542889399928334]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0r2i7_Nx6wh",
        "outputId": "2dc2cf64-121b-4ec9-ca18-5b350948aa88"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger# load the trained model\n",
        "\n",
        "model = SequenceTagger.load('/content/resources/taggers/example-ner/best-model.pt')# create example sentence\n",
        "sentence = Sentence(\"Olen Helsingissä\")# predict the tags\n",
        "sentence2 = Sentence(\"running in Seurasaari\")\n",
        "model.predict(sentence)\n",
        "model.predict(sentence2)\n",
        "print(sentence.to_tagged_string())\n",
        "print(sentence2.to_tagged_string())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-11 13:57:19,842 loading file /content/resources/taggers/example-ner/best-model.pt\n",
            "Olen Helsingissä <B-location>\n",
            "running in Seurasaari <B-location>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2ua-_YczOF6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pscg2sYzwp7"
      },
      "source": [
        "!zip -r /content/resources.zip /content/resources/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar3y1wwR2GxH"
      },
      "source": [
        "!cp resources.zip '/content/drive/MyDrive/Trained_model_geoparser/multilingual/'\n",
        "!ls -lt '/content/drive/MyDrive/Trained_model_geoparser/multilingual/' "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}